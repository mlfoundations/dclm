{
    "name": "/home/ubuntu/research/openlm/dcnlp/eval/heavy",
    "uuid": "f408c2a7-57c3-478a-a015-32fd5b39cc0b",
    "model": "d=1024_l=24_h=8",
    "creation_date": "2024_04_28-06_02_27",
    "eval_metrics": {
        "perplexity": 2.788094619909922,
        "downstream_perpexity": {
            "mmlu": 1.763707657966361,
            "hellaswag": 2.635287898419222,
            "jeopardy_all": 2.694533425116235,
            "triviaqa_sm_sub": 3.4066838223934175,
            "gsm8k": 2.3431414109093387,
            "agi_eval_sat_math": 1.8616275716911663,
            "aqua": 2.7284357070922853,
            "svamp": 2.8799798973401387,
            "bigbench_qa_wikidata": 4.874144531212643,
            "arc_easy": 2.7932719149774172,
            "arc_challenge": 2.897218492047372,
            "bigbench_misconceptions": 5.874304993511879,
            "copa": 2.829358811378479,
            "siqa": 1.4728765240589208,
            "commonsense_qa": 1.6912428441348377,
            "piqa": 2.8832983041873304,
            "openbook_qa": 4.491456831932068,
            "bigbench_novel_concepts": 3.1716100722551346,
            "bigbench_strange_stories": 3.5754331884713006,
            "bigbench_strategy_qa": 1.9254507536240257,
            "lambada_openai": 1.8735089518770347,
            "winograd_wsc": 2.800938058248806,
            "winogrande": 3.2783747122056295,
            "bigbench_conlang_translation": 2.358288224150495,
            "bigbench_language_identification": 2.1851554263661113,
            "bigbench_conceptual_combinations": 1.2658361699974652,
            "bigbench_elementary_math_qa": 3.0661082330521046,
            "bigbench_dyck_languages": 4.930527340888977,
            "agi_eval_lsat_ar": 1.6606917153234066,
            "bigbench_cs_algorithms": 5.711359848036911,
            "bigbench_logical_deduction": 1.5714684034188589,
            "bigbench_operators": 5.680341257367815,
            "bigbench_repeat_copy_logic": 1.8371348716318607,
            "simple_arithmetic_nospaces": 6.93111009311676,
            "simple_arithmetic_withspaces": 6.1881872091293335,
            "math_qa": 3.414310169987258,
            "logi_qa": 1.8278424944928897,
            "pubmed_qa_labeled": 4.488834334850312,
            "squad": 2.376869248170952,
            "agi_eval_lsat_rc": 1.5997000274373525,
            "agi_eval_lsat_lr": 1.5801599710595373,
            "coqa": 2.1563499452296346,
            "bigbench_understanding_fables": 1.913867298257414,
            "boolq": 3.467403661001713,
            "agi_eval_sat_en": 1.6358062896913694,
            "winogender_mc_female": 1.4336560487747192,
            "winogender_mc_male": 1.4840103884538014,
            "enterprise_pii_classification": 3.9662227044927176,
            "bbq": 0.4090438446167971,
            "human_eval_return_complex": 1.6628452173368198,
            "human_eval_return_simple": 3.3675204289926066,
            "human_eval-0.5": 1.6168674623093955,
            "human_eval-0.25": 1.6457245146355979,
            "human_eval-0.75": 1.640992102826514,
            "human_eval": 1.737245583679618,
            "processed_human_eval_cpp": 1.7004027522128562,
            "processed_human_eval_js": 1.6593513234359463
        },
        "icl": {
            "mmlu_zeroshot": 0.2437701452719538,
            "hellaswag_zeroshot": 0.38020315766334534,
            "jeopardy": 0.046628752164542674,
            "triviaqa_sm_sub": 0.06233333423733711,
            "gsm8k": 0.005307050887495279,
            "agi_eval_sat_math": 0.004545454401522875,
            "aqua": 0.0,
            "bigbench_qa_wikidata": 0.43649426102638245,
            "arc_easy": 0.5244107842445374,
            "arc_challenge": 0.2773037552833557,
            "mmlu_fewshot": 0.2602748151933938,
            "bigbench_misconceptions": 0.4794520437717438,
            "copa": 0.6100000143051147,
            "siqa": 0.5143295526504517,
            "commonsense_qa": 0.315315306186676,
            "piqa": 0.6784548163414001,
            "openbook_qa": 0.3400000035762787,
            "bigbench_novel_concepts": 0.3125,
            "bigbench_strange_stories": 0.5517241358757019,
            "bigbench_strategy_qa": 0.503276526927948,
            "lambada_openai": 0.43741509318351746,
            "hellaswag": 0.3791077435016632,
            "winograd": 0.6190476417541504,
            "winogrande": 0.507498025894165,
            "bigbench_conlang_translation": 0.012195121496915817,
            "bigbench_language_identification": 0.25760000944137573,
            "bigbench_conceptual_combinations": 0.26213592290878296,
            "bigbench_elementary_math_qa": 0.23862683773040771,
            "bigbench_dyck_languages": 0.20399999618530273,
            "agi_eval_lsat_ar": 0.239130437374115,
            "bigbench_cs_algorithms": 0.38712120056152344,
            "bigbench_logical_deduction": 0.27266666293144226,
            "bigbench_operators": 0.10476190596818924,
            "bigbench_repeat_copy_logic": 0.0,
            "simple_arithmetic_nospaces": 0.004000000189989805,
            "simple_arithmetic_withspaces": 0.004000000189989805,
            "math_qa": 0.24740193784236908,
            "logi_qa": 0.2841781973838806,
            "pubmed_qa_labeled": 0.5040000081062317,
            "squad": 0.14683064818382263,
            "agi_eval_lsat_rc": 0.24253731966018677,
            "agi_eval_lsat_lr": 0.2137254923582077,
            "coqa": 0.149066761136055,
            "bigbench_understanding_fables": 0.25925925374031067,
            "boolq": 0.47370031476020813,
            "agi_eval_sat_en": 0.22815534472465515,
            "winogender_mc_female": 0.44999998807907104,
            "winogender_mc_male": 0.4333333373069763,
            "enterprise_pii_classification": 0.48748159408569336,
            "bbq": 0.5170085430145264,
            "gpqa_main": 0.2053571492433548,
            "gpqa_diamond": 0.1818181872367859,
            "gsm8k_cot": 0.004548900760710239,
            "agi_eval_sat_math_cot": 0.004545454401522875,
            "aqua_cot": 0.004081632476300001,
            "svamp_cot": 0.04333333298563957
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.17044231552222947,
        "language understanding": 0.15598862337018185,
        "reading comprehension": 0.06627182761337934,
        "safety": -0.056088268756866455,
        "symbolic problem solving": 0.06847313045094545,
        "world knowledge": 0.07616068836029248
    },
    "aggregated_centered_results": 0.08879194146011257,
    "aggregated_results": 0.29332342267015477,
    "rw_small": 0.4621860086917877,
    "rw_small_centered": 0.07098119527275798,
    "95%_CI_above": 0.36168112184719314,
    "95%_CI_above_centered": 0.14693476083440252,
    "99%_CI_above": 0.3586074878907074,
    "99%_CI_above_centered": 0.16945107585809482,
    "low_variance_datasets": 0.3415495740334418,
    "low_variance_datasets_centered": 0.16371317666717716,
    "model_uuid": "6f571056-2344-4a1f-b09d-b7eaee375ab7",
    "_filename": "exp_data/evals/evaluation_cc_v4_resiliparse_rw_v2_bff1shards_shard_3_OH_eli5_vs_rw_v2_bigram_200k_train_0.1-d=1024_l=24_h=8-warm=2000-lr=0p003-wd=0p033-cd=3e-05-bs=512-mult=1-seed=124-tokens=8232325120_heavy.json",
    "missing tasks": "[]",
    "Core": 0.16371317666717716,
    "Extended": 0.08879194146011257
}