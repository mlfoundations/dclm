{
    "name": "/opt/ml/code/eval/heavy",
    "uuid": "4d540711-b572-4b7a-a3f6-0b533111586f",
    "model": "open_lm_1b_swiglutorch",
    "creation_date": "2024_04_29-11_36_33",
    "eval_metrics": {
        "perplexity": 2.7378819743792215,
        "downstream_perpexity": {
            "mmlu": 1.738102263491508,
            "hellaswag": 2.33436536750953,
            "jeopardy_all": 1.6836942582970584,
            "triviaqa_sm_sub": 2.3859866808056833,
            "gsm8k": 1.8526259077779266,
            "agi_eval_sat_math": 1.5407192956317555,
            "aqua": 2.2869330941414345,
            "svamp": 2.6297862577438353,
            "bigbench_qa_wikidata": 3.6700476552491983,
            "arc_easy": 2.2920588956838506,
            "arc_challenge": 2.4283333444656368,
            "bigbench_misconceptions": 4.466398362155374,
            "copa": 2.501029144525528,
            "siqa": 1.5366801255923088,
            "commonsense_qa": 1.9381761686612504,
            "piqa": 2.5411554820130258,
            "openbook_qa": 4.103561683654785,
            "bigbench_novel_concepts": 2.4135779216885567,
            "bigbench_strange_stories": 2.8865033093540147,
            "bigbench_strategy_qa": 1.7585917836768807,
            "lambada_openai": 1.2881922435467998,
            "winograd_wsc": 2.498626027570103,
            "winogrande": 3.0388692575076854,
            "bigbench_conlang_translation": 2.063210400866299,
            "bigbench_language_identification": 1.6462306711501564,
            "bigbench_conceptual_combinations": 1.1838153930543696,
            "bigbench_elementary_math_qa": 3.934225783736076,
            "bigbench_dyck_languages": 4.389406062841416,
            "agi_eval_lsat_ar": 1.6168926607007565,
            "bigbench_cs_algorithms": 4.028229111974889,
            "bigbench_logical_deduction": 1.3245991312265397,
            "bigbench_operators": 4.940718921025594,
            "bigbench_repeat_copy_logic": 1.354007214307785,
            "simple_arithmetic_nospaces": 6.903598271369934,
            "simple_arithmetic_withspaces": 5.791001720428467,
            "math_qa": 1.684135322078458,
            "logi_qa": 1.773721245393592,
            "pubmed_qa_labeled": 2.555921120405197,
            "squad": 1.874592353909672,
            "agi_eval_lsat_rc": 1.9445170524405009,
            "agi_eval_lsat_lr": 1.7957477555555457,
            "coqa": 1.682353895364303,
            "bigbench_understanding_fables": 1.5547642070780356,
            "boolq": 2.684465252223,
            "agi_eval_sat_en": 2.0039753601389023,
            "winogender_mc_female": 1.120572551091512,
            "winogender_mc_male": 0.8960216591755549,
            "enterprise_pii_classification": 3.581652272397183,
            "bbq": 0.2755778100524855,
            "human_eval_return_complex": 1.3339650964173746,
            "human_eval_return_simple": 2.817558430336617,
            "human_eval-0.5": 1.2570505963592995,
            "human_eval-0.25": 1.2628096352990081,
            "human_eval-0.75": 1.2987282425165176,
            "human_eval": 1.3284450366002758,
            "processed_human_eval_cpp": 1.6070016089433468,
            "processed_human_eval_js": 1.45267490061318
        },
        "icl": {
            "mmlu_zeroshot": 0.24532106989308408,
            "hellaswag_zeroshot": 0.5757817029953003,
            "jeopardy": 0.24852604866027833,
            "triviaqa_sm_sub": 0.19433332979679108,
            "gsm8k": 0.011372251436114311,
            "agi_eval_sat_math": 0.022727273404598236,
            "aqua": 0.008163264952600002,
            "bigbench_qa_wikidata": 0.5983957648277283,
            "arc_easy": 0.6489899158477783,
            "arc_challenge": 0.35238906741142273,
            "mmlu_fewshot": 0.24285069839996204,
            "bigbench_misconceptions": 0.4611872136592865,
            "copa": 0.7099999785423279,
            "siqa": 0.5163766741752625,
            "commonsense_qa": 0.24242424964904785,
            "piqa": 0.7459194660186768,
            "openbook_qa": 0.38999998569488525,
            "bigbench_novel_concepts": 0.5,
            "bigbench_strange_stories": 0.5977011322975159,
            "bigbench_strategy_qa": 0.5176933407783508,
            "lambada_openai": 0.6021735072135925,
            "hellaswag": 0.580163300037384,
            "winograd": 0.7765567898750305,
            "winogrande": 0.5848460793495178,
            "bigbench_conlang_translation": 0.018292682245373726,
            "bigbench_language_identification": 0.24690000712871552,
            "bigbench_conceptual_combinations": 0.21359223127365112,
            "bigbench_elementary_math_qa": 0.24310795962810516,
            "bigbench_dyck_languages": 0.21799999475479126,
            "agi_eval_lsat_ar": 0.23043477535247803,
            "bigbench_cs_algorithms": 0.42500001192092896,
            "bigbench_logical_deduction": 0.2433333396911621,
            "bigbench_operators": 0.20000000298023224,
            "bigbench_repeat_copy_logic": 0.03125,
            "simple_arithmetic_nospaces": 0.0020000000949949026,
            "simple_arithmetic_withspaces": 0.0020000000949949026,
            "math_qa": 0.2601408064365387,
            "logi_qa": 0.2611367106437683,
            "pubmed_qa_labeled": 0.4129999876022339,
            "squad": 0.3798486292362213,
            "agi_eval_lsat_rc": 0.2238806039094925,
            "agi_eval_lsat_lr": 0.27843138575553894,
            "coqa": 0.3011399209499359,
            "bigbench_understanding_fables": 0.19576719403266907,
            "boolq": 0.6131498217582703,
            "agi_eval_sat_en": 0.26213592290878296,
            "winogender_mc_female": 0.4000000059604645,
            "winogender_mc_male": 0.5333333611488342,
            "enterprise_pii_classification": 0.5089837908744812,
            "bbq": 0.4458235746080225,
            "gpqa_main": 0.2120535671710968,
            "gpqa_diamond": 0.21212121844291687,
            "gsm8k_cot": 0.008339650928974152,
            "agi_eval_sat_math_cot": 0.022727273404598236,
            "aqua_cot": 0.004081632476300001,
            "svamp_cot": 0.05666666850447655
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.25403154280217277,
        "language understanding": 0.29260359152591076,
        "reading comprehension": 0.15184274538861295,
        "safety": -0.055929633704098786,
        "symbolic problem solving": 0.07861298961845571,
        "world knowledge": 0.15152636201583852
    },
    "aggregated_centered_results": 0.1520483578745735,
    "aggregated_results": 0.3395906046611749,
    "rw_small": 0.5808594276507696,
    "rw_small_centered": 0.279091040990506,
    "95%_CI_above": 0.43982923745915486,
    "95%_CI_above_centered": 0.2540752505972295,
    "99%_CI_above": 0.45117273006750186,
    "99%_CI_above_centered": 0.2988155812979835,
    "low_variance_datasets": 0.4409949554638429,
    "low_variance_datasets_centered": 0.30060445268453945,
    "model_uuid": "7f73acae-a59b-4699-b731-9aad19706e81",
    "_filename": "exp_data/evals/evaluation_cc_v4_resiliparse_rw_v2_bff_minngram20_32shards_shard3_OH_eli5_vs_rw_v2_bigram_200k_train_0.1-open_lm_1b_swiglutorch-warm=5000-lr=0p003-wd=0p033-cd=3e-05-bs=256-mult=1-seed=124-tokens=28795904000_heavy.json",
    "missing tasks": "[]",
    "Core": 0.30060445268453945,
    "Extended": 0.1520483578745735
}