{
    "name": "heavy",
    "uuid": "a33bb7d9-0ac8-4b49-99d5-1e465bac0966",
    "model": "google/gemma-2b",
    "creation_date": "2024_02_25-20_28_08",
    "eval_metrics": {
        "icl": {
            "mmlu_zeroshot": 0.32991015126830653,
            "hellaswag_zeroshot": 0.694184422492981,
            "jeopardy": 0.4106133937835693,
            "triviaqa_sm_sub": 0.4153333306312561,
            "gsm8k": 0.0636846125125885,
            "agi_eval_sat_math": 0.036363635212183,
            "aqua": 0.044897958636283875,
            "bigbench_qa_wikidata": 0.7071502208709717,
            "arc_easy": 0.7420033812522888,
            "arc_challenge": 0.45392492413520813,
            "bigbench_misconceptions": 0.49771690368652344,
            "copa": 0.7799999713897705,
            "siqa": 0.6714431643486023,
            "commonsense_qa": 0.5904995799064636,
            "piqa": 0.7894450426101685,
            "openbook_qa": 0.3880000114440918,
            "bigbench_novel_concepts": 0.53125,
            "bigbench_strange_stories": 0.568965494632721,
            "bigbench_strategy_qa": 0.5670598745346069,
            "lambada_openai": 0.6371045708656311,
            "hellaswag": 0.7018522024154663,
            "winograd": 0.831501841545105,
            "winogrande": 0.6479873657226562,
            "bigbench_conlang_translation": 0.07926829159259796,
            "bigbench_language_identification": 0.29600000381469727,
            "bigbench_conceptual_combinations": 0.3300970792770386,
            "bigbench_elementary_math_qa": 0.2827044129371643,
            "bigbench_dyck_languages": 0.25699999928474426,
            "agi_eval_lsat_ar": 0.25217390060424805,
            "bigbench_cs_algorithms": 0.44924241304397583,
            "bigbench_logical_deduction": 0.24066667258739471,
            "bigbench_operators": 0.4571428596973419,
            "bigbench_repeat_copy_logic": 0.125,
            "simple_arithmetic_nospaces": 0.2770000100135803,
            "simple_arithmetic_withspaces": 0.28299999237060547,
            "math_qa": 0.2598055601119995,
            "logi_qa": 0.2549923062324524,
            "pubmed_qa_labeled": 0.3919999897480011,
            "squad": 0.5436140298843384,
            "agi_eval_lsat_rc": 0.28358209133148193,
            "agi_eval_lsat_lr": 0.3176470696926117,
            "coqa": 0.4256545305252075,
            "bigbench_understanding_fables": 0.3174603283405304,
            "boolq": 0.7492354512214661,
            "agi_eval_sat_en": 0.29611650109291077,
            "winogender_mc_female": 0.4166666567325592,
            "winogender_mc_male": 0.550000011920929,
            "enterprise_pii_classification": 0.6256259083747864,
            "bbq": 0.516351277177984,
            "mmlu_fewshot": 0.40849730869134265,
            "gsm8k_cot": 0.16906747221946716,
            "agi_eval_sat_math_cot": 0.145454540848732,
            "aqua_cot": 0.05714285746216774,
            "svamp_cot": 0.3166666626930237,
            "gpqa_main": 0.234375,
            "gpqa_diamond": 0.2222222238779068
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.37556734968485056,
        "language understanding": 0.40029817987827643,
        "reading comprehension": 0.2701515465844096,
        "safety": 0.054321927103129275,
        "symbolic problem solving": 0.18735643372143784,
        "world knowledge": 0.2716441404958915
    },
    "aggregated_centered_results": 0.2662681552523263,
    "aggregated_results": 0.42995130681018257,
    "rw_small": 0.6778892527023951,
    "95%_CI_above": 0.5303823767157344,
    "99%_CI_above": 0.5500415656877601,
    "low_variance_datasets": 0.5422422780231996,
    "_filename": "exp_data/evals/gemma2b_heavy.json",
    "missing tasks": "[]",
    "rw_small_centered": 0.4527030451255932,
    "95%_CI_above_centered": 0.37931499132678864,
    "99%_CI_above_centered": 0.4327694492450113,
    "low_variance_datasets_centered": 0.4334530946631591,
    "Core": 0.4334530946631591,
    "Extended": 0.2662681552523263
}