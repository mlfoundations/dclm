{
    "name": "/opt/ml/code/eval/heavy",
    "uuid": "d0366456-7feb-4493-9c8b-8ca277d7ba7e",
    "model": "open_lm_1b_swiglutorch",
    "creation_date": "2024_06_01-01_02_24",
    "eval_metrics": {
        "perplexity": 2.027463134129842,
        "downstream_perpexity": {
            "mmlu": 2.1173008106624516,
            "hellaswag": 2.3629889225394716,
            "jeopardy_all": 1.7132857206242795,
            "triviaqa_sm_sub": 1.9278128749132157,
            "gsm8k": 1.7514904163568827,
            "agi_eval_sat_math": 1.431463033502752,
            "aqua": 2.2139178859944244,
            "svamp": 2.2339086437225344,
            "bigbench_qa_wikidata": 3.1854360854494885,
            "arc_easy": 2.374281387889024,
            "arc_challenge": 2.5139537657588824,
            "bigbench_misconceptions": 4.7785680174283245,
            "copa": 2.5290301561355593,
            "siqa": 1.9576535104119057,
            "commonsense_qa": 2.255190642713817,
            "piqa": 2.583705936474432,
            "openbook_qa": 4.178642535686493,
            "bigbench_novel_concepts": 2.3716566935181618,
            "bigbench_strange_stories": 2.986409067422494,
            "bigbench_strategy_qa": 1.8607687167383897,
            "lambada_openai": 1.2377672599057086,
            "winograd_wsc": 2.4813447509493147,
            "winogrande": 3.0964266632896007,
            "bigbench_conlang_translation": 1.8104529264496594,
            "bigbench_language_identification": 1.640406456428138,
            "bigbench_conceptual_combinations": 0.7759259267918115,
            "bigbench_elementary_math_qa": 3.823059348555856,
            "bigbench_dyck_languages": 4.504736294746399,
            "agi_eval_lsat_ar": 2.019504256352134,
            "bigbench_cs_algorithms": 4.211381488677227,
            "bigbench_logical_deduction": 0.8805015190442403,
            "bigbench_operators": 4.949838508878435,
            "bigbench_repeat_copy_logic": 1.3236563336104155,
            "simple_arithmetic_nospaces": 6.813151802539825,
            "simple_arithmetic_withspaces": 5.5147234511375425,
            "math_qa": 1.9862358170070387,
            "logi_qa": 2.1234890057743967,
            "pubmed_qa_labeled": 3.9064759879112243,
            "squad": 1.636488855556147,
            "agi_eval_lsat_rc": 1.8597225373360649,
            "agi_eval_lsat_lr": 1.997587143206129,
            "coqa": 1.2744718545752183,
            "bigbench_understanding_fables": 1.859860832729037,
            "boolq": 3.370815595063959,
            "agi_eval_sat_en": 1.9769891662505066,
            "winogender_mc_female": 1.049033742149671,
            "winogender_mc_male": 0.807377823193868,
            "enterprise_pii_classification": 4.816225425062896,
            "bbq": 0.309263889239452,
            "human_eval_return_complex": 0.8124942286746708,
            "human_eval_return_simple": 3.619643933064229,
            "human_eval-0.5": 0.7039804469521452,
            "human_eval-0.25": 0.7526246131193347,
            "human_eval-0.75": 0.71817453023864,
            "human_eval": 0.8188161937201895,
            "processed_human_eval_cpp": 0.7540785562918053,
            "processed_human_eval_js": 0.7305528003631568
        },
        "icl": {
            "mmlu_zeroshot": 0.2583971888872615,
            "hellaswag_zeroshot": 0.5732921957969666,
            "jeopardy": 0.3251898378133774,
            "triviaqa_sm_sub": 0.2736666798591614,
            "gsm8k_cot": 0.008339650928974152,
            "agi_eval_sat_math_cot": 0.027272727340459824,
            "aqua_cot": 0.016326529905200005,
            "bigbench_qa_wikidata": 0.6693568229675293,
            "arc_easy": 0.627525269985199,
            "arc_challenge": 0.3199658691883087,
            "mmlu_fewshot": 0.24810715098130076,
            "bigbench_misconceptions": 0.5068492889404297,
            "copa": 0.7200000286102295,
            "siqa": 0.5051177144050598,
            "commonsense_qa": 0.21048320829868317,
            "piqa": 0.7361262440681458,
            "openbook_qa": 0.3700000047683716,
            "bigbench_novel_concepts": 0.53125,
            "bigbench_strange_stories": 0.5747126340866089,
            "bigbench_strategy_qa": 0.5216251611709595,
            "lambada_openai": 0.6176984310150146,
            "hellaswag": 0.5818561911582947,
            "winograd": 0.8058608174324036,
            "winogrande": 0.6014207005500793,
            "bigbench_conlang_translation": 0.024390242993831635,
            "bigbench_language_identification": 0.2524000108242035,
            "bigbench_conceptual_combinations": 0.26213592290878296,
            "bigbench_elementary_math_qa": 0.23983228206634521,
            "bigbench_dyck_languages": 0.2549999952316284,
            "agi_eval_lsat_ar": 0.22608695924282074,
            "bigbench_cs_algorithms": 0.405303031206131,
            "bigbench_logical_deduction": 0.25,
            "bigbench_operators": 0.19523809850215912,
            "bigbench_repeat_copy_logic": 0.09375,
            "simple_arithmetic_nospaces": 0.004000000189989805,
            "simple_arithmetic_withspaces": 0.00800000037997961,
            "math_qa": 0.25879988074302673,
            "logi_qa": 0.25806450843811035,
            "pubmed_qa_labeled": 0.24400000274181366,
            "squad": 0.4454115331172943,
            "agi_eval_lsat_rc": 0.25,
            "agi_eval_lsat_lr": 0.250980406999588,
            "coqa": 0.3422272205352783,
            "bigbench_understanding_fables": 0.23280423879623413,
            "boolq": 0.4743119180202484,
            "agi_eval_sat_en": 0.26213592290878296,
            "winogender_mc_female": 0.36666667461395264,
            "winogender_mc_male": 0.46666666865348816,
            "enterprise_pii_classification": 0.5101619958877563,
            "bbq": 0.4775045243176547,
            "gpqa_main": 0.2522321343421936,
            "gpqa_diamond": 0.2373737394809723,
            "svamp_cot": 0.07000000029802322
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.24376777505459285,
        "language understanding": 0.3154902363511579,
        "reading comprehension": 0.10395763809547612,
        "safety": -0.08950006826357407,
        "symbolic problem solving": 0.08568526320712275,
        "world knowledge": 0.18733803890079087
    },
    "aggregated_centered_results": 0.15295064641642017,
    "aggregated_results": 0.34426260870940206,
    "rw_small": 0.5527526636918386,
    "rw_small_centered": 0.21452714703236408,
    "95%_CI_above": 0.4428873747490443,
    "95%_CI_above_centered": 0.25160631144683754,
    "99%_CI_above": 0.4500115815712058,
    "99%_CI_above_centered": 0.2883836351056575,
    "low_variance_datasets": 0.44765929037874397,
    "low_variance_datasets_centered": 0.29754806275437223,
    "model_uuid": "da93bee4-338a-4717-aa3b-4aa6da31cc15",
    "_filename": "exp_data/evals/evaluation_rpj_original-open_lm_1b_swiglutorch-warm=5000-lr=0p003-wd=0p033-cd=3e-05-bs=256-mult=5p0-seed=124-tokens=143979520000_heavy.json",
    "missing tasks": "[]",
    "Core": 0.29754806275437223,
    "Extended": 0.15295064641642017
}