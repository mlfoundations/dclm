{
    "name": "/mnt/task_runtime/dcnlp/eval/heavy",
    "uuid": "03c583bd-7c2c-4438-9c92-19446e505323",
    "model": "open_lm_1b",
    "creation_date": "2024_01_31-12_04_58",
    "eval_metrics": {
        "perplexity": 2.7212232768535616,
        "downstream_perpexity": {
            "mmlu": 1.5303030544545742,
            "hellaswag": 2.3308068083781355,
            "jeopardy_all": 2.110928262580832,
            "triviaqa_sm_sub": 2.4811416059533755,
            "gsm8k": 2.028997012301771,
            "agi_eval_sat_math": 1.9466875943270596,
            "aqua": 2.5852626187460763,
            "svamp": 2.6151546184221903,
            "bigbench_qa_wikidata": 3.530511025632497,
            "arc_easy": 2.5896419105104322,
            "arc_challenge": 2.7275863409856074,
            "bigbench_misconceptions": 5.826579548996877,
            "copa": 2.510756229162216,
            "siqa": 1.2794266380331534,
            "commonsense_qa": 1.5532894941942308,
            "piqa": 2.644370626962224,
            "openbook_qa": 4.2429387288093565,
            "bigbench_novel_concepts": 2.6024922877550125,
            "bigbench_strange_stories": 3.09778203498358,
            "bigbench_strategy_qa": 1.7288084692598795,
            "lambada_openai": 1.5246844131021668,
            "winograd_wsc": 2.580805521308284,
            "winogrande": 3.073044245696576,
            "bigbench_conlang_translation": 2.0331855459911066,
            "bigbench_language_identification": 2.066734290857462,
            "bigbench_conceptual_combinations": 1.1783515793605916,
            "bigbench_elementary_math_qa": 3.5595572791134535,
            "bigbench_dyck_languages": 5.097425150394439,
            "agi_eval_lsat_ar": 1.6594573451125103,
            "bigbench_cs_algorithms": 5.507507233547442,
            "bigbench_logical_deduction": 1.020937251051267,
            "bigbench_operators": 5.192171332949684,
            "bigbench_repeat_copy_logic": 1.5901782475411892,
            "simple_arithmetic_nospaces": 6.841774945259094,
            "simple_arithmetic_withspaces": 6.128595510005951,
            "math_qa": 2.1334581927409157,
            "logi_qa": 1.6843205571357738,
            "pubmed_qa_labeled": 4.229901800632477,
            "squad": 1.821324572353399,
            "agi_eval_lsat_rc": 1.6456119854058793,
            "agi_eval_lsat_lr": 1.6266925540624881,
            "coqa": 1.9746520876302078,
            "bigbench_understanding_fables": 1.673346859437448,
            "boolq": 3.311614808613372,
            "agi_eval_sat_en": 1.5927547459463471,
            "winogender_mc_female": 1.1675346165895462,
            "winogender_mc_male": 1.01451136469841,
            "enterprise_pii_classification": 3.8395972676059458,
            "bbq": 0.2880417284120929,
            "human_eval_return_complex": 3.1858063344880354,
            "human_eval_return_simple": 6.30971266772296,
            "human_eval-0.5": 3.089863867294498,
            "human_eval-0.25": 3.215596510142815,
            "human_eval-0.75": 3.1720768482219883,
            "human_eval": 3.383649839133751,
            "processed_human_eval_cpp": 3.1593861772406915,
            "processed_human_eval_js": 3.0937168874391694
        },
        "icl": {
            "mmlu_zeroshot": 0.2428328130851712,
            "hellaswag_zeroshot": 0.5785700082778931,
            "jeopardy": 0.13768653869628905,
            "triviaqa_sm_sub": 0.00033333332976326346,
            "gsm8k": 0.0,
            "agi_eval_sat_math": 0.004545454401522875,
            "aqua": 0.0,
            "bigbench_qa_wikidata": 0.5928841829299927,
            "arc_easy": 0.5753366947174072,
            "arc_challenge": 0.2994880676269531,
            "bigbench_misconceptions": 0.4840182662010193,
            "copa": 0.699999988079071,
            "siqa": 0.49129989743232727,
            "commonsense_qa": 0.2588042616844177,
            "piqa": 0.7415668964385986,
            "openbook_qa": 0.3619999885559082,
            "bigbench_novel_concepts": 0.4375,
            "bigbench_strange_stories": 0.540229856967926,
            "bigbench_strategy_qa": 0.5028396844863892,
            "lambada_openai": 0.530758798122406,
            "hellaswag": 0.5833499431610107,
            "winograd": 0.721611738204956,
            "winogrande": 0.5682715177536011,
            "bigbench_conlang_translation": 0.018292682245373726,
            "bigbench_language_identification": 0.2517000138759613,
            "bigbench_conceptual_combinations": 0.20388349890708923,
            "bigbench_elementary_math_qa": 0.2493448704481125,
            "bigbench_dyck_languages": 0.16699999570846558,
            "agi_eval_lsat_ar": 0.25652173161506653,
            "bigbench_cs_algorithms": 0.3803030252456665,
            "bigbench_logical_deduction": 0.2666666805744171,
            "bigbench_operators": 0.1666666716337204,
            "bigbench_repeat_copy_logic": 0.03125,
            "simple_arithmetic_nospaces": 0.0010000000474974513,
            "simple_arithmetic_withspaces": 0.0010000000474974513,
            "math_qa": 0.25175997614860535,
            "logi_qa": 0.26420891284942627,
            "pubmed_qa_labeled": 0.2549999952316284,
            "squad": 0.33509933948516846,
            "agi_eval_lsat_rc": 0.24626865983009338,
            "agi_eval_lsat_lr": 0.29411765933036804,
            "coqa": 0.2640611231327057,
            "bigbench_understanding_fables": 0.2539682686328888,
            "boolq": 0.5501528978347778,
            "agi_eval_sat_en": 0.25242719054222107,
            "winogender_mc_female": 0.4833333194255829,
            "winogender_mc_male": 0.46666666865348816,
            "enterprise_pii_classification": 0.47835052013397217,
            "bbq": 0.5050074376843192,
            "mmlu_fewshot": 0.24406567477343374,
            "gsm8k_cot": 0.02047005295753479,
            "agi_eval_sat_math_cot": 0.004545454401522875,
            "aqua_cot": 0.016326529905200005,
            "svamp_cot": 0.05000000074505806,
            "gpqa_main": 0.2142857164144516,
            "gpqa_diamond": 0.2070707082748413
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.13870900810713344,
        "language understanding": 0.22817637862299397,
        "reading comprehension": 0.09159099983803014,
        "safety": -0.033321027051318786,
        "symbolic problem solving": 0.06348341965737442,
        "world knowledge": 0.10763801538810944
    },
    "aggregated_centered_results": 0.10496525345437159,
    "aggregated_results": 0.32075844816013693,
    "rw_small": 0.5453031957149506,
    "95%_CI_above": 0.4088633377927465,
    "99%_CI_above": 0.4145536488812903,
    "model_uuid": "3d6614b0-7827-4bdf-8888-dd774e4fe510",
    "low_variance_datasets": 0.41150379194454717,
    "_filename": "exp_data/evals/evaluation_rw_pagerank_bucket_0_of_5-open_lm_1b-warm=5000-lr=0p003-wd=0p033-cd=3e-05-bs=256-mult=1-seed=124_heavy.json",
    "missing tasks": "[]",
    "rw_small_centered": 0.21509909037260985,
    "95%_CI_above_centered": 0.19613321610047116,
    "99%_CI_above_centered": 0.23457931187217254,
    "low_variance_datasets_centered": 0.23572226466947022,
    "Core": 0.23572226466947022,
    "Extended": 0.10496525345437159,
    "Core_v1": 0.2607912082313814,
    "Extended_v1": 0.12874750505541407,
    "Core_v2": 0.23572226466947022,
    "Extended_v2": 0.10496525345437159,
    "eval_version": "v2"
}