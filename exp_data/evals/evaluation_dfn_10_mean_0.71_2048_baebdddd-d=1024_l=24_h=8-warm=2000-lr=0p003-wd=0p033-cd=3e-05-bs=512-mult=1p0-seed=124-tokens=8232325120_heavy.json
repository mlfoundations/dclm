{
    "name": "/opt/ml/code/eval/heavy",
    "uuid": "051ad0a8-6957-4080-85bb-6cf37917f176",
    "model": "d=1024_l=24_h=8",
    "creation_date": "2024_06_04-01_34_09",
    "eval_metrics": {
        "perplexity": 2.7324699838956197,
        "downstream_perpexity": {
            "mmlu": 1.7993865815378496,
            "hellaswag": 2.64485495911013,
            "jeopardy_all": 3.1043538676718243,
            "triviaqa_sm_sub": 3.503272771636645,
            "gsm8k": 2.5532825956568743,
            "agi_eval_sat_math": 2.0488739571788095,
            "aqua": 2.917853081956202,
            "svamp": 3.057128012975057,
            "bigbench_qa_wikidata": 4.879077934349424,
            "arc_easy": 3.21491329457222,
            "arc_challenge": 3.1935966390188235,
            "bigbench_misconceptions": 5.802420191568871,
            "copa": 2.867379466295242,
            "siqa": 1.4987309827043187,
            "commonsense_qa": 1.7017336447252591,
            "piqa": 2.845816311561244,
            "openbook_qa": 4.689716208457947,
            "bigbench_novel_concepts": 3.3516501039266586,
            "bigbench_strange_stories": 3.760106530682794,
            "bigbench_strategy_qa": 2.096987325221287,
            "lambada_openai": 2.0003393127496394,
            "winograd_wsc": 2.839945690099136,
            "winogrande": 3.316703695712733,
            "bigbench_conlang_translation": 2.5233320913663726,
            "bigbench_language_identification": 3.104887592897436,
            "bigbench_conceptual_combinations": 1.0772725199032755,
            "bigbench_elementary_math_qa": 3.135430365629041,
            "bigbench_dyck_languages": 5.756305003166199,
            "agi_eval_lsat_ar": 1.7244782655135444,
            "bigbench_cs_algorithms": 5.690122296593406,
            "bigbench_logical_deduction": 1.1032276885906855,
            "bigbench_operators": 6.129975570951189,
            "bigbench_repeat_copy_logic": 1.954358670860529,
            "simple_arithmetic_nospaces": 7.740469789028167,
            "simple_arithmetic_withspaces": 6.5681980862617495,
            "math_qa": 4.773585254619316,
            "logi_qa": 1.8189141325503817,
            "pubmed_qa_labeled": 5.8879228501319885,
            "squad": 2.3582797222805114,
            "agi_eval_lsat_rc": 1.7530907303539676,
            "agi_eval_lsat_lr": 1.7403001163520064,
            "coqa": 2.320227343628733,
            "bigbench_understanding_fables": 3.778990136252509,
            "boolq": 3.6163210669788746,
            "agi_eval_sat_en": 1.880021423969454,
            "winogender_mc_female": 1.884496549765269,
            "winogender_mc_male": 1.8621053139368693,
            "enterprise_pii_classification": 5.475746211668529,
            "bbq": 0.4061271580385663,
            "human_eval_return_complex": 2.082657755829218,
            "human_eval_return_simple": 3.1216664378707475,
            "human_eval-0.5": 1.9712636790624478,
            "human_eval-0.25": 2.0182042470792445,
            "human_eval-0.75": 2.018066044260816,
            "human_eval": 2.1219504155763764,
            "processed_human_eval_cpp": 1.984657072872849,
            "processed_human_eval_js": 1.8582805206136006
        },
        "icl": {
            "mmlu_zeroshot": 0.23709610365984732,
            "hellaswag_zeroshot": 0.3859788775444031,
            "jeopardy": 0.025137170869857072,
            "triviaqa_sm_sub": 0.05966666713356972,
            "gsm8k_cot": 0.0030326005071401596,
            "agi_eval_sat_math_cot": 0.00909090880304575,
            "aqua_cot": 0.004081632476300001,
            "bigbench_qa_wikidata": 0.4319177269935608,
            "arc_easy": 0.48400673270225525,
            "arc_challenge": 0.24317406117916107,
            "mmlu_fewshot": 0.25999374949095544,
            "bigbench_misconceptions": 0.4885844886302948,
            "copa": 0.6100000143051147,
            "siqa": 0.501023530960083,
            "commonsense_qa": 0.24570024013519287,
            "piqa": 0.6789988875389099,
            "openbook_qa": 0.31200000643730164,
            "bigbench_novel_concepts": 0.34375,
            "bigbench_strange_stories": 0.49425286054611206,
            "bigbench_strategy_qa": 0.5155089497566223,
            "lambada_openai": 0.41684454679489136,
            "hellaswag": 0.38807010650634766,
            "winograd": 0.6263736486434937,
            "winogrande": 0.49802684783935547,
            "bigbench_conlang_translation": 0.012195121496915817,
            "bigbench_language_identification": 0.2533000111579895,
            "bigbench_conceptual_combinations": 0.25242719054222107,
            "bigbench_elementary_math_qa": 0.25500524044036865,
            "bigbench_dyck_languages": 0.16699999570846558,
            "agi_eval_lsat_ar": 0.21739129722118378,
            "bigbench_cs_algorithms": 0.3893939256668091,
            "bigbench_logical_deduction": 0.25,
            "bigbench_operators": 0.12857143580913544,
            "bigbench_repeat_copy_logic": 0.0,
            "simple_arithmetic_nospaces": 0.003000000026077032,
            "simple_arithmetic_withspaces": 0.004000000189989805,
            "math_qa": 0.23801541328430176,
            "logi_qa": 0.2519201338291168,
            "pubmed_qa_labeled": 0.42100000381469727,
            "squad": 0.13812677562236786,
            "agi_eval_lsat_rc": 0.24253731966018677,
            "agi_eval_lsat_lr": 0.2803921699523926,
            "coqa": 0.1439308524131775,
            "bigbench_understanding_fables": 0.23280423879623413,
            "boolq": 0.47951069474220276,
            "agi_eval_sat_en": 0.23786407709121704,
            "winogender_mc_female": 0.4333333373069763,
            "winogender_mc_male": 0.46666666865348816,
            "enterprise_pii_classification": 0.5116347670555115,
            "bbq": 0.4839421537789432,
            "gpqa_main": 0.2165178507566452,
            "gpqa_diamond": 0.2222222238779068,
            "svamp_cot": 0.05999999865889549
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.14611378018518556,
        "language understanding": 0.15312792847191195,
        "reading comprehension": 0.06397935797117257,
        "safety": -0.05221153660254044,
        "symbolic problem solving": 0.06292789977120414,
        "world knowledge": 0.07112381711466052
    },
    "aggregated_centered_results": 0.08211469913831447,
    "aggregated_results": 0.2878304388114572,
    "rw_small": 0.45011620471874875,
    "rw_small_centered": 0.055092986912755236,
    "95%_CI_above": 0.3496548944089392,
    "95%_CI_above_centered": 0.13318494839641568,
    "99%_CI_above": 0.3453386354584085,
    "99%_CI_above_centered": 0.15530954495357943,
    "low_variance_datasets": 0.3301569934468716,
    "low_variance_datasets_centered": 0.1502908119575955,
    "model_uuid": "44b914ca-68d3-4b86-89fb-beaa2b4bc242",
    "_filename": "exp_data/evals/evaluation_dfn_10_mean_0.71_2048_baebdddd-d=1024_l=24_h=8-warm=2000-lr=0p003-wd=0p033-cd=3e-05-bs=512-mult=1p0-seed=124-tokens=8232325120_heavy.json",
    "missing tasks": "[]",
    "Core": 0.1502908119575955,
    "Extended": 0.08211469913831447
}