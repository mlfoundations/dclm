{
    "name": "/home/ubuntu/research/openlm/dcnlp/eval/heavy",
    "uuid": "8a355ca9-3a78-4f09-931b-cbd6ccafc28b",
    "model": "d=1024_l=24_h=8",
    "creation_date": "2024_04_28-18_49_44",
    "eval_metrics": {
        "perplexity": 2.788185332218806,
        "downstream_perpexity": {
            "mmlu": 1.6980520820553155,
            "hellaswag": 2.6563392523820837,
            "jeopardy_all": 2.738139659433811,
            "triviaqa_sm_sub": 3.454462586005529,
            "gsm8k": 2.360579585468706,
            "agi_eval_sat_math": 1.8602472982623361,
            "aqua": 2.741090068038629,
            "svamp": 2.812265325387319,
            "bigbench_qa_wikidata": 4.979036869891756,
            "arc_easy": 2.8874422072962878,
            "arc_challenge": 2.990120256567571,
            "bigbench_misconceptions": 5.594319742019862,
            "copa": 2.858866991996765,
            "siqa": 1.6563398942366594,
            "commonsense_qa": 1.66626602191597,
            "piqa": 2.895010949608032,
            "openbook_qa": 4.498438244819641,
            "bigbench_novel_concepts": 3.0699621438980103,
            "bigbench_strange_stories": 3.430596313257327,
            "bigbench_strategy_qa": 1.8234599737581492,
            "lambada_openai": 1.9162184229677635,
            "winograd_wsc": 2.800031911322485,
            "winogrande": 3.2800385827799903,
            "bigbench_conlang_translation": 2.408870583627282,
            "bigbench_language_identification": 2.383242443516913,
            "bigbench_conceptual_combinations": 1.254065436066933,
            "bigbench_elementary_math_qa": 3.1701471344042624,
            "bigbench_dyck_languages": 4.671105108976364,
            "agi_eval_lsat_ar": 1.631634976034579,
            "bigbench_cs_algorithms": 5.323927961696278,
            "bigbench_logical_deduction": 1.1945873834292093,
            "bigbench_operators": 5.994535591488793,
            "bigbench_repeat_copy_logic": 1.8426399268209934,
            "simple_arithmetic_nospaces": 7.298826417446136,
            "simple_arithmetic_withspaces": 5.949370153903962,
            "math_qa": 2.8520848062107955,
            "logi_qa": 1.8131930846406201,
            "pubmed_qa_labeled": 4.205856939792633,
            "squad": 2.384224549530809,
            "agi_eval_lsat_rc": 1.655720381149605,
            "agi_eval_lsat_lr": 1.6756220338391323,
            "coqa": 2.1580017902312623,
            "bigbench_understanding_fables": 1.801221560548853,
            "boolq": 3.0873836460463497,
            "agi_eval_sat_en": 1.7395226480891404,
            "winogender_mc_female": 1.6883940041065215,
            "winogender_mc_male": 1.880959689617157,
            "enterprise_pii_classification": 2.957822878265943,
            "bbq": 0.36094423642815493,
            "human_eval_return_complex": 1.688849958847827,
            "human_eval_return_simple": 3.2296175119039177,
            "human_eval-0.5": 1.5989731513872378,
            "human_eval-0.25": 1.636374326740823,
            "human_eval-0.75": 1.639916255706694,
            "human_eval": 1.7327975827019388,
            "processed_human_eval_cpp": 1.7066924675651218,
            "processed_human_eval_js": 1.6932224842106425
        },
        "icl": {
            "mmlu_zeroshot": 0.24910675851922287,
            "hellaswag_zeroshot": 0.37472614645957947,
            "jeopardy": 0.04326457232236862,
            "triviaqa_sm_sub": 0.06966666877269745,
            "gsm8k": 0.0037907506339251995,
            "agi_eval_sat_math": 0.004545454401522875,
            "aqua": 0.004081632476300001,
            "bigbench_qa_wikidata": 0.42601248621940613,
            "arc_easy": 0.5147306323051453,
            "arc_challenge": 0.2645051181316376,
            "mmlu_fewshot": 0.25482408310237686,
            "bigbench_misconceptions": 0.5114155411720276,
            "copa": 0.6000000238418579,
            "siqa": 0.5127942562103271,
            "commonsense_qa": 0.4021294116973877,
            "piqa": 0.6670293807983398,
            "openbook_qa": 0.3100000023841858,
            "bigbench_novel_concepts": 0.375,
            "bigbench_strange_stories": 0.477011501789093,
            "bigbench_strategy_qa": 0.5015290379524231,
            "lambada_openai": 0.4279060661792755,
            "hellaswag": 0.3740290701389313,
            "winograd": 0.6410256624221802,
            "winogrande": 0.5106551051139832,
            "bigbench_conlang_translation": 0.018292682245373726,
            "bigbench_language_identification": 0.2551000118255615,
            "bigbench_conceptual_combinations": 0.21359223127365112,
            "bigbench_elementary_math_qa": 0.2390461266040802,
            "bigbench_dyck_languages": 0.17900000512599945,
            "agi_eval_lsat_ar": 0.28260868787765503,
            "bigbench_cs_algorithms": 0.3962121307849884,
            "bigbench_logical_deduction": 0.2666666805744171,
            "bigbench_operators": 0.12380952388048172,
            "bigbench_repeat_copy_logic": 0.03125,
            "simple_arithmetic_nospaces": 0.004000000189989805,
            "simple_arithmetic_withspaces": 0.0010000000474974513,
            "math_qa": 0.24371437728405,
            "logi_qa": 0.26728111505508423,
            "pubmed_qa_labeled": 0.2930000126361847,
            "squad": 0.1255439966917038,
            "agi_eval_lsat_rc": 0.2761194109916687,
            "agi_eval_lsat_lr": 0.250980406999588,
            "coqa": 0.16196918487548828,
            "bigbench_understanding_fables": 0.21164020895957947,
            "boolq": 0.5801223516464233,
            "agi_eval_sat_en": 0.2330097109079361,
            "winogender_mc_female": 0.5166666507720947,
            "winogender_mc_male": 0.4833333194255829,
            "enterprise_pii_classification": 0.5346097350120544,
            "bbq": 0.5064740424806421,
            "gpqa_main": 0.2321428507566452,
            "gpqa_diamond": 0.2070707082748413,
            "gsm8k_cot": 0.0037907506339251995,
            "agi_eval_sat_math_cot": 0.004545454401522875,
            "aqua_cot": 0.0,
            "svamp_cot": 0.06333333253860474
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.09524209989574886,
        "language understanding": 0.1142863728747754,
        "reading comprehension": 0.05473812476715498,
        "safety": 0.020541873845187075,
        "symbolic problem solving": 0.06238139884598139,
        "world knowledge": 0.08582816777783528
    },
    "aggregated_centered_results": 0.07528873464434445,
    "aggregated_results": 0.2964771174774672,
    "rw_small": 0.47412322958310443,
    "rw_small_centered": 0.10916415676038864,
    "95%_CI_above": 0.3617148476181092,
    "95%_CI_above_centered": 0.13813021167700387,
    "99%_CI_above": 0.3593104046324025,
    "99%_CI_above_centered": 0.16676845874264065,
    "low_variance_datasets": 0.34961952594193546,
    "low_variance_datasets_centered": 0.15819341291762987,
    "model_uuid": "4be94f05-f4f4-4592-b1ec-635c135a3687",
    "_filename": "exp_data/evals/evaluation_rw_v2_cc_v3_f0.15_resiliparse_fasttext_openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train_0.1-d=1024_l=24_h=8-warm=2000-lr=0p003-wd=0p033-cd=3e-05-bs=512-mult=1-seed=124-tokens=8232325120_heavy.json",
    "missing tasks": "[]",
    "Core": 0.15819341291762987,
    "Extended": 0.07528873464434445,
    "Core_v1": 0.18035844454836708,
    "Extended_v1": 0.09783736948814989,
    "Core_v2": 0.15819341291762987,
    "Extended_v2": 0.07528873464434445,
    "eval_version": "v2"
}