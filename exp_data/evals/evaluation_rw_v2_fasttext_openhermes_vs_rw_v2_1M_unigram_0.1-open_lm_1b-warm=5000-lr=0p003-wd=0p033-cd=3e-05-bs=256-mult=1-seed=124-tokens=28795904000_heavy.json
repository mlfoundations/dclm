{
    "name": "/mnt/task_runtime/dcnlp/eval/heavy",
    "uuid": "6043312e-94ca-4ea4-9a01-94cfd2236465",
    "model": "open_lm_1b",
    "creation_date": "2024_02_18-16_11_24",
    "eval_metrics": {
        "perplexity": 2.5585999846458436,
        "downstream_perpexity": {
            "mmlu": 1.606373542675417,
            "hellaswag": 2.336498894341983,
            "jeopardy_all": 1.4880790563760413,
            "triviaqa_sm_sub": 2.2647576098243394,
            "gsm8k": 1.918590327661267,
            "agi_eval_sat_math": 1.5675521856004542,
            "aqua": 2.3033107017984196,
            "svamp": 2.4729735883076986,
            "bigbench_qa_wikidata": 3.690997083498839,
            "arc_easy": 2.230752766935111,
            "arc_challenge": 2.421528133287161,
            "bigbench_misconceptions": 4.79993971406597,
            "copa": 2.4518671464920043,
            "siqa": 1.3704144050545435,
            "commonsense_qa": 1.719030017251367,
            "piqa": 2.6250021931136653,
            "openbook_qa": 4.073725933551788,
            "bigbench_novel_concepts": 2.4660951048135757,
            "bigbench_strange_stories": 2.8670653413081992,
            "bigbench_strategy_qa": 1.692297670350631,
            "lambada_openai": 1.3220824664894202,
            "winograd_wsc": 2.454334237636664,
            "winogrande": 3.064579610290166,
            "bigbench_conlang_translation": 1.985410478783817,
            "bigbench_language_identification": 3.497144806549582,
            "bigbench_conceptual_combinations": 0.8937521461144234,
            "bigbench_elementary_math_qa": 4.065659996251765,
            "bigbench_dyck_languages": 4.099948343038559,
            "agi_eval_lsat_ar": 1.74435758901679,
            "bigbench_cs_algorithms": 5.11721044959444,
            "bigbench_logical_deduction": 0.8083872271776199,
            "bigbench_operators": 4.909656137511844,
            "bigbench_repeat_copy_logic": 1.5044964663684368,
            "simple_arithmetic_nospaces": 6.994407924175262,
            "simple_arithmetic_withspaces": 6.415162234306336,
            "math_qa": 4.025454764792953,
            "logi_qa": 1.741112949112044,
            "pubmed_qa_labeled": 3.906327758073807,
            "squad": 2.033723949574026,
            "agi_eval_lsat_rc": 1.6455366544759096,
            "agi_eval_lsat_lr": 1.689466917514801,
            "coqa": 2.6030606096375934,
            "bigbench_understanding_fables": 2.89192207906612,
            "boolq": 3.5615375190699865,
            "agi_eval_sat_en": 1.6717650878776624,
            "winogender_mc_female": 1.1468660295009614,
            "winogender_mc_male": 1.1685949563980103,
            "enterprise_pii_classification": 4.609913955205319,
            "bbq": 0.3278429916322513,
            "human_eval_return_complex": 2.46348581163902,
            "human_eval_return_simple": 5.56966833166174,
            "human_eval-0.5": 2.2939665034049894,
            "human_eval-0.25": 2.386742247313988,
            "human_eval-0.75": 2.369868765516979,
            "human_eval": 2.5289177414847583,
            "processed_human_eval_cpp": 2.49314576646556,
            "processed_human_eval_js": 2.321005872110041
        },
        "icl": {
            "mmlu_zeroshot": 0.2586418782409869,
            "hellaswag_zeroshot": 0.5856403112411499,
            "jeopardy": 0.31372723579406736,
            "triviaqa_sm_sub": 0.0006666666595265269,
            "gsm8k": 0.0,
            "agi_eval_sat_math": 0.00909090880304575,
            "aqua": 0.0,
            "bigbench_qa_wikidata": 0.6206879615783691,
            "arc_easy": 0.6582491397857666,
            "arc_challenge": 0.3498293459415436,
            "bigbench_misconceptions": 0.4611872136592865,
            "copa": 0.75,
            "siqa": 0.4744114577770233,
            "commonsense_qa": 0.312039315700531,
            "piqa": 0.7551686763763428,
            "openbook_qa": 0.39399999380111694,
            "bigbench_novel_concepts": 0.5,
            "bigbench_strange_stories": 0.5747126340866089,
            "bigbench_strategy_qa": 0.5198776721954346,
            "lambada_openai": 0.5868425965309143,
            "hellaswag": 0.5881298780441284,
            "winograd": 0.791208803653717,
            "winogrande": 0.5769534111022949,
            "bigbench_conlang_translation": 0.024390242993831635,
            "bigbench_language_identification": 0.2540000081062317,
            "bigbench_conceptual_combinations": 0.3106796145439148,
            "bigbench_elementary_math_qa": 0.24177148938179016,
            "bigbench_dyck_languages": 0.17599999904632568,
            "agi_eval_lsat_ar": 0.27826085686683655,
            "bigbench_cs_algorithms": 0.44999998807907104,
            "bigbench_logical_deduction": 0.23866666853427887,
            "bigbench_operators": 0.19523809850215912,
            "bigbench_repeat_copy_logic": 0.0,
            "simple_arithmetic_nospaces": 0.0020000000949949026,
            "simple_arithmetic_withspaces": 0.004000000189989805,
            "math_qa": 0.25142472982406616,
            "logi_qa": 0.27496159076690674,
            "pubmed_qa_labeled": 0.4259999990463257,
            "squad": 0.38382214307785034,
            "agi_eval_lsat_rc": 0.25746268033981323,
            "agi_eval_lsat_lr": 0.23725490272045135,
            "coqa": 0.27721408009529114,
            "bigbench_understanding_fables": 0.20105819404125214,
            "boolq": 0.631192684173584,
            "agi_eval_sat_en": 0.21844659745693207,
            "winogender_mc_female": 0.5666666626930237,
            "winogender_mc_male": 0.5333333611488342,
            "enterprise_pii_classification": 0.4839469790458679,
            "bbq": 0.4831996424631639,
            "mmlu_fewshot": 0.24524102634505224,
            "gsm8k_cot": 0.012130402028560638,
            "agi_eval_sat_math_cot": 0.013636363670229912,
            "aqua_cot": 0.008163264952600002,
            "svamp_cot": 0.06333333253860474,
            "gpqa_main": 0.1941964328289032,
            "gpqa_diamond": 0.18686868250370026
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.264822285344032,
        "language understanding": 0.3132677018566608,
        "reading comprehension": 0.14926739653016916,
        "safety": 0.033573322675444855,
        "symbolic problem solving": 0.07959685530279631,
        "world knowledge": 0.13814916322118062
    },
    "aggregated_centered_results": 0.1609169211755175,
    "aggregated_results": 0.3433308473635706,
    "rw_small": 0.5919774820407232,
    "95%_CI_above": 0.45260250062330976,
    "99%_CI_above": 0.46018049989057624,
    "model_uuid": "6fd29db9-af86-4d31-a7c9-300bc67865e2",
    "low_variance_datasets": 0.4512820239771496,
    "_filename": "exp_data/evals/evaluation_rw_v2_fasttext_openhermes_vs_rw_v2_1M_unigram_0.1-open_lm_1b-warm=5000-lr=0p003-wd=0p033-cd=3e-05-bs=256-mult=1-seed=124-tokens=28795904000_heavy.json",
    "missing tasks": "[]",
    "rw_small_centered": 0.30138660836638065,
    "95%_CI_above_centered": 0.2731454713092795,
    "99%_CI_above_centered": 0.3121398136929007,
    "low_variance_datasets_centered": 0.3165745317504355,
    "Core": 0.3165745317504355,
    "Extended": 0.1609169211755175
}