{
    "name": "/opt/ml/code/eval/heavy",
    "uuid": "31e2aac1-f600-4984-b793-094d5a716a3b",
    "model": "open_lm_7b",
    "creation_date": "2024_04_10-16_22_05",
    "eval_metrics": {
        "perplexity": 2.1562233448028563,
        "downstream_perpexity": {
            "mmlu": 1.5957051938525157,
            "hellaswag": 2.138496471636088,
            "jeopardy_all": 0.9831963529793593,
            "triviaqa_sm_sub": 1.5690152714873353,
            "gsm8k": 1.5232252801702817,
            "agi_eval_sat_math": 1.292485794695941,
            "aqua": 1.9416442039061566,
            "svamp": 2.5048554865519206,
            "bigbench_qa_wikidata": 2.9379217949342578,
            "arc_easy": 1.8893695766927818,
            "arc_challenge": 2.0901589198324055,
            "bigbench_misconceptions": 4.708811755594053,
            "copa": 2.268559538125992,
            "siqa": 1.4849907245216252,
            "commonsense_qa": 1.835947965423559,
            "piqa": 2.319706066907814,
            "openbook_qa": 3.8096033701896665,
            "bigbench_novel_concepts": 2.0832231044769287,
            "bigbench_strange_stories": 2.621119553330301,
            "bigbench_strategy_qa": 1.6421266258309741,
            "lambada_openai": 0.9489729065515279,
            "winograd_wsc": 2.324176216300154,
            "winogrande": 2.915287348446925,
            "bigbench_conlang_translation": 1.6002742473672076,
            "bigbench_language_identification": 1.6594519112367205,
            "bigbench_conceptual_combinations": 0.7488078440277322,
            "bigbench_elementary_math_qa": 3.238602328356707,
            "bigbench_dyck_languages": 2.766572009086609,
            "agi_eval_lsat_ar": 1.779772288384645,
            "bigbench_cs_algorithms": 3.029250878456867,
            "bigbench_logical_deduction": 1.0333120361963908,
            "bigbench_operators": 4.340461209842137,
            "bigbench_repeat_copy_logic": 1.1542316488921642,
            "simple_arithmetic_nospaces": 6.063183187246323,
            "simple_arithmetic_withspaces": 5.287644991397857,
            "math_qa": 1.8242555473860262,
            "logi_qa": 1.830430598119803,
            "pubmed_qa_labeled": 3.28605726313591,
            "squad": 1.5710557096180495,
            "agi_eval_lsat_rc": 1.6097705524359176,
            "agi_eval_lsat_lr": 1.7335460143930772,
            "coqa": 1.1335889028267918,
            "bigbench_understanding_fables": 1.6367236703791945,
            "boolq": 3.755151789195676,
            "agi_eval_sat_en": 1.6269933274648722,
            "winogender_mc_female": 0.9208097954591116,
            "winogender_mc_male": 0.8677995701630911,
            "enterprise_pii_classification": 4.671930540503388,
            "bbq": 0.333302370019095,
            "human_eval_return_complex": 0.9807631021409523,
            "human_eval_return_simple": 2.835589956592869,
            "human_eval-0.5": 0.9384065472134729,
            "human_eval-0.25": 0.9489845571721472,
            "human_eval-0.75": 0.9760913805263799,
            "human_eval": 1.0004384176033299,
            "processed_human_eval_cpp": 1.350104302353,
            "processed_human_eval_js": 1.2449053834851196
        },
        "icl": {
            "mmlu_zeroshot": 0.2702315930734601,
            "hellaswag_zeroshot": 0.7189803123474121,
            "jeopardy": 0.4492002367973328,
            "triviaqa_sm_sub": 0.43700000643730164,
            "gsm8k": 0.015921153128147125,
            "agi_eval_sat_math": 0.013636363670229912,
            "aqua": 0.01224489789456129,
            "bigbench_qa_wikidata": 0.691206157207489,
            "arc_easy": 0.744528591632843,
            "arc_challenge": 0.45904436707496643,
            "mmlu_fewshot": 0.2837409220243755,
            "bigbench_misconceptions": 0.5388127565383911,
            "copa": 0.8100000023841858,
            "siqa": 0.5404298901557922,
            "commonsense_qa": 0.2588042616844177,
            "piqa": 0.781283974647522,
            "openbook_qa": 0.42399999499320984,
            "bigbench_novel_concepts": 0.53125,
            "bigbench_strange_stories": 0.6839080452919006,
            "bigbench_strategy_qa": 0.5591961741447449,
            "lambada_openai": 0.7044439911842346,
            "hellaswag": 0.7316271662712097,
            "winograd": 0.8461538553237915,
            "winogrande": 0.6724545955657959,
            "bigbench_conlang_translation": 0.024390242993831635,
            "bigbench_language_identification": 0.2556999921798706,
            "bigbench_conceptual_combinations": 0.3106796145439148,
            "bigbench_elementary_math_qa": 0.2504192888736725,
            "bigbench_dyck_languages": 0.16500000655651093,
            "agi_eval_lsat_ar": 0.22608695924282074,
            "bigbench_cs_algorithms": 0.45606061816215515,
            "bigbench_logical_deduction": 0.2280000001192093,
            "bigbench_operators": 0.21904762089252472,
            "bigbench_repeat_copy_logic": 0.09375,
            "simple_arithmetic_nospaces": 0.02500000037252903,
            "simple_arithmetic_withspaces": 0.02199999988079071,
            "math_qa": 0.23835065960884094,
            "logi_qa": 0.25806450843811035,
            "pubmed_qa_labeled": 0.46299999952316284,
            "squad": 0.49526962637901306,
            "agi_eval_lsat_rc": 0.302238792181015,
            "agi_eval_lsat_lr": 0.24117647111415863,
            "coqa": 0.4012276232242584,
            "bigbench_understanding_fables": 0.26455026865005493,
            "boolq": 0.6602446436882019,
            "agi_eval_sat_en": 0.2669903039932251,
            "winogender_mc_female": 0.5333333611488342,
            "winogender_mc_male": 0.44999998807907104,
            "enterprise_pii_classification": 0.5611193180084229,
            "bbq": 0.43469353968446905,
            "gpqa_main": 0.2254464328289032,
            "gpqa_diamond": 0.2070707082748413,
            "gsm8k_cot": 0.016679301857948303,
            "agi_eval_sat_math_cot": 0.022727273404598236,
            "aqua_cot": 0.016326529905200005,
            "svamp_cot": 0.10000000149011612
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.3374166942412394,
        "language understanding": 0.41195275463118775,
        "reading comprehension": 0.21850185747372738,
        "safety": -0.01042689653960141,
        "symbolic problem solving": 0.08658749992580718,
        "world knowledge": 0.25751154000647586
    },
    "aggregated_centered_results": 0.21839982572759806,
    "aggregated_results": 0.38813095452982366,
    "rw_small": 0.6775420854489008,
    "rw_small_centered": 0.43903675239685686,
    "95%_CI_above": 0.5045149199858114,
    "95%_CI_above_centered": 0.34272222702577165,
    "99%_CI_above": 0.523153648817021,
    "99%_CI_above_centered": 0.39694888107734355,
    "low_variance_datasets": 0.5120052089745348,
    "low_variance_datasets_centered": 0.3966260790658078,
    "model_uuid": "ee84406e-1ac9-44cd-b69e-a6358c79b696",
    "_filename": "exp_data/evals/evaluation_cc_v4_resiliparse_rw_v2_bff_minngram20_10shards_shard3_OH_eli5_vs_rw_v2_bigram_200k_train_0.1-open_lm_7b-warm=5000-lr=0p0003-wd=0p33-cd=3e-05-bs=2048-mult=1-seed=124-tokens=137788211200_heavy.json",
    "missing tasks": "[]",
    "Core": 0.3966260790658078,
    "Extended": 0.21839982572759806
}