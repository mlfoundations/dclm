{
    "name": "/opt/ml/code/eval/heavy",
    "uuid": "d9966d06-fcfb-4f08-855a-f216bce6db1e",
    "model": "open_lm_1b_swiglutorch",
    "creation_date": "2024_04_28-20_44_38",
    "eval_metrics": {
        "perplexity": 2.7187557379404703,
        "downstream_perpexity": {
            "mmlu": 1.7936614405529874,
            "hellaswag": 2.324106791726481,
            "jeopardy_all": 1.6056257124583733,
            "triviaqa_sm_sub": 2.326584843357404,
            "gsm8k": 1.8547389789274735,
            "agi_eval_sat_math": 1.6164286071603948,
            "aqua": 2.384545976775033,
            "svamp": 2.701299005349477,
            "bigbench_qa_wikidata": 3.3724393923814344,
            "arc_easy": 2.4268715445341082,
            "arc_challenge": 2.5679069827390206,
            "bigbench_misconceptions": 5.176402342373922,
            "copa": 2.507450872659683,
            "siqa": 1.3179932649084343,
            "commonsense_qa": 1.717042800054308,
            "piqa": 2.523719452670142,
            "openbook_qa": 4.109472829818726,
            "bigbench_novel_concepts": 2.4193244948983192,
            "bigbench_strange_stories": 2.8782920590762435,
            "bigbench_strategy_qa": 1.6882236408010327,
            "lambada_openai": 1.2910881816253554,
            "winograd_wsc": 2.4991677621782045,
            "winogrande": 3.052866678493834,
            "bigbench_conlang_translation": 2.0468761928197816,
            "bigbench_language_identification": 1.5344862381227733,
            "bigbench_conceptual_combinations": 1.3555687723807919,
            "bigbench_elementary_math_qa": 5.200481260210213,
            "bigbench_dyck_languages": 4.066599529743194,
            "agi_eval_lsat_ar": 1.8354566195736761,
            "bigbench_cs_algorithms": 3.6928578618801002,
            "bigbench_logical_deduction": 1.3875695809523265,
            "bigbench_operators": 5.231682100182488,
            "bigbench_repeat_copy_logic": 1.4384098798036575,
            "simple_arithmetic_nospaces": 6.472580924034118,
            "simple_arithmetic_withspaces": 5.528511532306672,
            "math_qa": 1.6487123698066075,
            "logi_qa": 1.9510718521800825,
            "pubmed_qa_labeled": 4.109078396558761,
            "squad": 1.971328150994843,
            "agi_eval_lsat_rc": 1.9689395983717335,
            "agi_eval_lsat_lr": 1.6977808339923035,
            "coqa": 1.7953280483020304,
            "bigbench_understanding_fables": 1.5366289142578367,
            "boolq": 3.191428508452319,
            "agi_eval_sat_en": 2.2106024275705654,
            "winogender_mc_female": 1.0808935701847076,
            "winogender_mc_male": 0.8414699703454971,
            "enterprise_pii_classification": 3.6374862913179467,
            "bbq": 0.30314321205242095,
            "human_eval_return_complex": 1.4380473104987557,
            "human_eval_return_simple": 2.726488023190885,
            "human_eval-0.5": 1.3648616864186962,
            "human_eval-0.25": 1.3829210633184852,
            "human_eval-0.75": 1.3857020212382805,
            "human_eval": 1.4619460273079756,
            "processed_human_eval_cpp": 1.436572345887652,
            "processed_human_eval_js": 1.3688988460273277
        },
        "icl": {
            "mmlu_zeroshot": 0.25751711114456777,
            "hellaswag_zeroshot": 0.5882294178009033,
            "jeopardy": 0.24949570298194884,
            "triviaqa_sm_sub": 0.19633333384990692,
            "gsm8k": 0.011372251436114311,
            "agi_eval_sat_math": 0.00909090880304575,
            "aqua": 0.008163264952600002,
            "bigbench_qa_wikidata": 0.6070075035095215,
            "arc_easy": 0.6405723690986633,
            "arc_challenge": 0.3242320716381073,
            "mmlu_fewshot": 0.25709318748691623,
            "bigbench_misconceptions": 0.5479452013969421,
            "copa": 0.7300000190734863,
            "siqa": 0.5092118978500366,
            "commonsense_qa": 0.20720720291137695,
            "piqa": 0.745375394821167,
            "openbook_qa": 0.36800000071525574,
            "bigbench_novel_concepts": 0.53125,
            "bigbench_strange_stories": 0.545976996421814,
            "bigbench_strategy_qa": 0.5246832966804504,
            "lambada_openai": 0.6043081879615784,
            "hellaswag": 0.5943039059638977,
            "winograd": 0.7509157657623291,
            "winogrande": 0.5816890001296997,
            "bigbench_conlang_translation": 0.018292682245373726,
            "bigbench_language_identification": 0.25270000100135803,
            "bigbench_conceptual_combinations": 0.3786407709121704,
            "bigbench_elementary_math_qa": 0.24114255607128143,
            "bigbench_dyck_languages": 0.23499999940395355,
            "agi_eval_lsat_ar": 0.239130437374115,
            "bigbench_cs_algorithms": 0.39772728085517883,
            "bigbench_logical_deduction": 0.23266667127609253,
            "bigbench_operators": 0.20000000298023224,
            "bigbench_repeat_copy_logic": 0.0,
            "simple_arithmetic_nospaces": 0.0,
            "simple_arithmetic_withspaces": 0.0010000000474974513,
            "math_qa": 0.2531009018421173,
            "logi_qa": 0.28110599517822266,
            "pubmed_qa_labeled": 0.31200000643730164,
            "squad": 0.37095552682876587,
            "agi_eval_lsat_rc": 0.26492536067962646,
            "agi_eval_lsat_lr": 0.250980406999588,
            "coqa": 0.2948766052722931,
            "bigbench_understanding_fables": 0.21164020895957947,
            "boolq": 0.582874596118927,
            "agi_eval_sat_en": 0.25242719054222107,
            "winogender_mc_female": 0.4833333194255829,
            "winogender_mc_male": 0.5166666507720947,
            "enterprise_pii_classification": 0.49455082416534424,
            "bbq": 0.466027785431255,
            "gpqa_main": 0.2232142835855484,
            "gpqa_diamond": 0.20202019810676575,
            "gsm8k_cot": 0.008339650928974152,
            "agi_eval_sat_math_cot": 0.00909090880304575,
            "aqua_cot": 0.004081632476300001,
            "svamp_cot": 0.07000000029802322
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.24208275072727126,
        "language understanding": 0.31840795643009834,
        "reading comprehension": 0.13035030401589578,
        "safety": -0.019710710102861573,
        "symbolic problem solving": 0.07653359173484504,
        "world knowledge": 0.16882592378826866
    },
    "aggregated_centered_results": 0.15630468609207618,
    "aggregated_results": 0.34169547211730944,
    "rw_small": 0.5745995789766312,
    "rw_small_centered": 0.26606523642065927,
    "95%_CI_above": 0.43929141678593375,
    "95%_CI_above_centered": 0.2537854588445029,
    "99%_CI_above": 0.4399326879045238,
    "99%_CI_above_centered": 0.2836423245006441,
    "low_variance_datasets": 0.43475459055467086,
    "low_variance_datasets_centered": 0.29093452721033203,
    "model_uuid": "b6c926c2-dcd6-448c-b0eb-6c4e93fd14a4",
    "_filename": "exp_data/evals/evaluation_rw_v2_cc_v3_f0.15_resiliparse_fasttext_openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train_0.2-open_lm_1b_swiglutorch-warm=5000-lr=0p003-wd=0p033-cd=3e-05-bs=256-mult=1-seed=124-tokens=28795904000_heavy.json",
    "missing tasks": "[]",
    "Core": 0.29093452721033203,
    "Extended": 0.15630468609207618
}