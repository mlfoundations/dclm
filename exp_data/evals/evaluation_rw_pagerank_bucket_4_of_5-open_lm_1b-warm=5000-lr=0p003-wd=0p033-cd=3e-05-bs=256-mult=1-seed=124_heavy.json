{
    "name": "/mnt/task_runtime/dcnlp/eval/heavy",
    "uuid": "237fcb2b-ddf5-489d-b103-8071f0f0ebee",
    "model": "open_lm_1b",
    "creation_date": "2024_01_28-19_17_28",
    "eval_metrics": {
        "icl": {
            "mmlu_zeroshot": 0.23143679792420907,
            "hellaswag_zeroshot": 0.5371440052986145,
            "jeopardy": 0.1741696447134018,
            "triviaqa_sm_sub": 0.00033333332976326346,
            "gsm8k": 0.0,
            "agi_eval_sat_math": 0.00909090880304575,
            "aqua": 0.0,
            "svamp": 0.0,
            "bigbench_qa_wikidata": 0.6134048700332642,
            "arc_easy": 0.5900673270225525,
            "arc_challenge": 0.3046075105667114,
            "bigbench_misconceptions": 0.4885844886302948,
            "copa": 0.6700000166893005,
            "siqa": 0.48157626390457153,
            "commonsense_qa": 0.34316134452819824,
            "piqa": 0.7247007489204407,
            "openbook_qa": 0.36000001430511475,
            "bigbench_novel_concepts": 0.40625,
            "bigbench_strange_stories": 0.522988498210907,
            "bigbench_strategy_qa": 0.5024027824401855,
            "lambada_openai": 0.53017657995224,
            "hellaswag": 0.5399323105812073,
            "winograd": 0.7509157657623291,
            "winogrande": 0.5682715177536011,
            "bigbench_conlang_translation": 0.018292682245373726,
            "bigbench_language_identification": 0.2529999911785126,
            "bigbench_conceptual_combinations": 0.20388349890708923,
            "bigbench_elementary_math_qa": 0.24166665971279144,
            "bigbench_dyck_languages": 0.17399999499320984,
            "agi_eval_lsat_ar": 0.24782608449459076,
            "bigbench_cs_algorithms": 0.4075757563114166,
            "bigbench_logical_deduction": 0.27133333683013916,
            "bigbench_operators": 0.17619048058986664,
            "bigbench_repeat_copy_logic": 0.0,
            "simple_arithmetic_nospaces": 0.0010000000474974513,
            "simple_arithmetic_withspaces": 0.0020000000949949026,
            "math_qa": 0.2594703435897827,
            "logi_qa": 0.24116744101047516,
            "pubmed_qa_labeled": 0.44600000977516174,
            "squad": 0.33197730779647827,
            "agi_eval_lsat_rc": 0.26119402050971985,
            "agi_eval_lsat_lr": 0.239215686917305,
            "coqa": 0.26681697368621826,
            "bigbench_understanding_fables": 0.2222222238779068,
            "boolq": 0.6165137887001038,
            "agi_eval_sat_en": 0.27184465527534485,
            "winogender_mc_female": 0.5,
            "winogender_mc_male": 0.550000011920929,
            "enterprise_pii_classification": 0.4972017705440521,
            "bbq": 0.4187008088285273,
            "mmlu_fewshot": 0.25995733392866033
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.13226304821033574,
        "language understanding": 0.22150566519831136,
        "reading comprehension": 0.12853193747108443,
        "safety": -0.01704870435324582,
        "symbolic problem solving": 0.0761601991746857,
        "world knowledge": 0.15997943140748289
    },
    "aggregated_centered_results": 0.12569804322992073,
    "aggregated_results": 0.35572712090070335,
    "rw_small": 0.5477673610051473,
    "95%_CI_above": 0.4158235126501554,
    "99%_CI_above": 0.42915886187035107,
    "model_uuid": "1e1fcb87-3c56-4fe4-80af-2dbcdf210eaa",
    "low_variance_datasets": 0.4172932742671533,
    "_filename": "exp_data/evals/evaluation_rw_pagerank_bucket_4_of_5-open_lm_1b-warm=5000-lr=0p003-wd=0p033-cd=3e-05-bs=256-mult=1-seed=124_heavy.json",
    "missing tasks": "['gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'gpqa_main', 'gpqa_diamond']",
    "rw_small_centered": 0.22941010319001495,
    "95%_CI_above_centered": 0.20439468879982953,
    "99%_CI_above_centered": 0.25532700432889555,
    "low_variance_datasets_centered": 0.24709625167906124,
    "Core": 0.24709625167906124,
    "Core_v1": 0.27055456612664597,
    "Core_v2": 0.24709625167906124,
    "Extended_v2": "N/A due to missing tasks: ['gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'gpqa_main', 'gpqa_diamond']",
    "eval_version": "v2",
    "Extended_v1": "N/A due to missing tasks: ['gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'gpqa_main', 'gpqa_diamond']",
    "Extended": "N/A due to missing tasks: ['gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'gpqa_main', 'gpqa_diamond']"
}