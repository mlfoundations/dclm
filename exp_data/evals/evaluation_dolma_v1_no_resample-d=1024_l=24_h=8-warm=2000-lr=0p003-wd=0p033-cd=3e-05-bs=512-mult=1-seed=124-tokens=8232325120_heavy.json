{
    "name": "/opt/ml/code/eval/heavy",
    "uuid": "869f2584-8552-45cf-8e5d-82be91071edf",
    "model": "d=1024_l=24_h=8",
    "creation_date": "2024_05_28-22_23_32",
    "eval_metrics": {
        "perplexity": 2.635071172316869,
        "downstream_perpexity": {
            "mmlu": 5.073012835686405,
            "hellaswag": 2.687172015690039,
            "jeopardy_all": 3.374001378657179,
            "triviaqa_sm_sub": 3.62687286567688,
            "gsm8k": 2.5314834940334086,
            "agi_eval_sat_math": 2.0309601827101273,
            "aqua": 3.0344112075105008,
            "svamp": 3.2563326501846315,
            "bigbench_qa_wikidata": 4.369207838999991,
            "arc_easy": 3.3155241933735935,
            "arc_challenge": 3.2828475650870352,
            "bigbench_misconceptions": 6.093868468994419,
            "copa": 3.0092262053489685,
            "siqa": 5.278808012711477,
            "commonsense_qa": 6.205762884638331,
            "piqa": 2.941899354361865,
            "openbook_qa": 4.731112008571625,
            "bigbench_novel_concepts": 3.397040009498596,
            "bigbench_strange_stories": 4.222054000558524,
            "bigbench_strategy_qa": 2.234478025365574,
            "lambada_openai": 2.328953695281626,
            "winograd_wsc": 2.8962753201142335,
            "winogrande": 3.346169816290382,
            "bigbench_conlang_translation": 2.455223824192838,
            "bigbench_language_identification": 4.215294608284793,
            "bigbench_conceptual_combinations": 0.9294800839377838,
            "bigbench_elementary_math_qa": 3.3721103917190365,
            "bigbench_dyck_languages": 4.402221486330032,
            "agi_eval_lsat_ar": 5.031405469645625,
            "bigbench_cs_algorithms": 6.656296730041504,
            "bigbench_logical_deduction": 0.7784389140605926,
            "bigbench_operators": 6.027442991165888,
            "bigbench_repeat_copy_logic": 2.014447219669819,
            "simple_arithmetic_nospaces": 7.44825252199173,
            "simple_arithmetic_withspaces": 5.999770462989807,
            "math_qa": 4.5505015008448755,
            "logi_qa": 3.5988457501392395,
            "pubmed_qa_labeled": 5.150300098896027,
            "squad": 2.460701795964164,
            "agi_eval_lsat_rc": 4.9449485540390015,
            "agi_eval_lsat_lr": 4.831395384844612,
            "coqa": 2.3682685840866045,
            "bigbench_understanding_fables": 5.054058443301569,
            "boolq": 4.495392440582998,
            "agi_eval_sat_en": 5.122002851615831,
            "winogender_mc_female": 1.5584757804870606,
            "winogender_mc_male": 1.8206217726071676,
            "enterprise_pii_classification": 4.31398388184223,
            "bbq": 0.47545574038124344,
            "human_eval_return_complex": 1.2143717910361103,
            "human_eval_return_simple": 3.725583405108065,
            "human_eval-0.5": 1.088502775605132,
            "human_eval-0.25": 1.1341385630572713,
            "human_eval-0.75": 1.1031103922826488,
            "human_eval": 1.2112140691861875,
            "processed_human_eval_cpp": 1.0438223366411576,
            "processed_human_eval_js": 0.9975091509702729
        },
        "icl": {
            "mmlu_zeroshot": 0.23328980646635356,
            "hellaswag_zeroshot": 0.36018720269203186,
            "jeopardy": 0.007306566229090095,
            "triviaqa_sm_sub": 0.043666668236255646,
            "gsm8k_cot": 0.004548900760710239,
            "agi_eval_sat_math_cot": 0.004545454401522875,
            "aqua_cot": 0.0,
            "bigbench_qa_wikidata": 0.36548396944999695,
            "arc_easy": 0.44528618454933167,
            "arc_challenge": 0.2542662024497986,
            "mmlu_fewshot": 0.2524146560514182,
            "bigbench_misconceptions": 0.5251141786575317,
            "copa": 0.5699999928474426,
            "siqa": 0.48874104022979736,
            "commonsense_qa": 0.30057328939437866,
            "piqa": 0.6768226623535156,
            "openbook_qa": 0.31200000643730164,
            "bigbench_novel_concepts": 0.40625,
            "bigbench_strange_stories": 0.5287356376647949,
            "bigbench_strategy_qa": 0.5019659399986267,
            "lambada_openai": 0.3415486216545105,
            "hellaswag": 0.3630750775337219,
            "winograd": 0.5750916004180908,
            "winogrande": 0.4909234344959259,
            "bigbench_conlang_translation": 0.012195121496915817,
            "bigbench_language_identification": 0.2574999928474426,
            "bigbench_conceptual_combinations": 0.2330097109079361,
            "bigbench_elementary_math_qa": 0.24958071112632751,
            "bigbench_dyck_languages": 0.23600000143051147,
            "agi_eval_lsat_ar": 0.20434781908988953,
            "bigbench_cs_algorithms": 0.489393949508667,
            "bigbench_logical_deduction": 0.27933332324028015,
            "bigbench_operators": 0.15238095819950104,
            "bigbench_repeat_copy_logic": 0.0,
            "simple_arithmetic_nospaces": 0.003000000026077032,
            "simple_arithmetic_withspaces": 0.004000000189989805,
            "math_qa": 0.2534361481666565,
            "logi_qa": 0.23963133990764618,
            "pubmed_qa_labeled": 0.5410000085830688,
            "squad": 0.06726584583520889,
            "agi_eval_lsat_rc": 0.23880596458911896,
            "agi_eval_lsat_lr": 0.272549033164978,
            "coqa": 0.1226356029510498,
            "bigbench_understanding_fables": 0.26455026865005493,
            "boolq": 0.47706422209739685,
            "agi_eval_sat_en": 0.24757280945777893,
            "winogender_mc_female": 0.5666666626930237,
            "winogender_mc_male": 0.4833333194255829,
            "enterprise_pii_classification": 0.5546391606330872,
            "bbq": 0.5017899356105111,
            "gpqa_main": 0.2321428507566452,
            "gpqa_diamond": 0.21717171370983124,
            "svamp_cot": 0.06666667014360428
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.0814951906601588,
        "language understanding": 0.0810869022791473,
        "reading comprehension": 0.04825734682054374,
        "safety": 0.053214539181102416,
        "symbolic problem solving": 0.06607924740140637,
        "world knowledge": 0.0646114113208244
    },
    "aggregated_centered_results": 0.06673351884967071,
    "aggregated_results": 0.29282075919643263,
    "rw_small": 0.43295453985532123,
    "rw_small_centered": 0.026447698386789054,
    "95%_CI_above": 0.34964068947350846,
    "95%_CI_above_centered": 0.11260935646954946,
    "99%_CI_above": 0.34379532081884856,
    "99%_CI_above_centered": 0.13460014792301989,
    "low_variance_datasets": 0.321325145566582,
    "low_variance_datasets_centered": 0.11161477558379414,
    "model_uuid": "fdca1b79-e7f5-4728-b216-13e91897eb1b",
    "_filename": "exp_data/evals/evaluation_dolma_v1_no_resample-d=1024_l=24_h=8-warm=2000-lr=0p003-wd=0p033-cd=3e-05-bs=512-mult=1-seed=124-tokens=8232325120_heavy.json",
    "missing tasks": "[]",
    "Core": 0.11161477558379414,
    "Extended": 0.06673351884967071,
    "Core_v1": 0.13601288049934543,
    "Extended_v1": 0.09021491158233788,
    "Core_v2": 0.11161477558379414,
    "Extended_v2": 0.06673351884967071,
    "eval_version": "v2"
}