{
    "name": "/opt/ml/code/eval/heavy",
    "uuid": "e6a55445-76dd-4112-9703-10190d663fb5",
    "model": "open_lm_7b_swiglutorch",
    "creation_date": "2024_04_26-03_34_40",
    "eval_metrics": {
        "perplexity": 2.368659661213557,
        "downstream_perpexity": {
            "mmlu": 1.644346960342061,
            "hellaswag": 2.098424756261035,
            "jeopardy_all": 0.938274114213648,
            "triviaqa_sm_sub": 1.5089554362520576,
            "gsm8k": 1.3837498020637748,
            "agi_eval_sat_math": 1.2389082599769938,
            "aqua": 1.8450362254162225,
            "svamp": 2.366606210867564,
            "bigbench_qa_wikidata": 3.1689807416023417,
            "arc_easy": 1.8273065242452253,
            "arc_challenge": 2.018358591676979,
            "bigbench_misconceptions": 2.7323859924595104,
            "copa": 2.1937734842300416,
            "siqa": 1.4852454952110166,
            "commonsense_qa": 1.6512068199584173,
            "piqa": 2.2784790137257747,
            "openbook_qa": 3.76559473323822,
            "bigbench_novel_concepts": 2.1145246103405952,
            "bigbench_strange_stories": 2.4444045965698944,
            "bigbench_strategy_qa": 1.7103922615513734,
            "lambada_openai": 0.8855344005038952,
            "winograd_wsc": 2.2326541243892013,
            "winogrande": 2.850332813180142,
            "bigbench_conlang_translation": 1.508226635252557,
            "bigbench_language_identification": 1.532198886509823,
            "bigbench_conceptual_combinations": 0.9222176780978453,
            "bigbench_elementary_math_qa": 3.4543923591386596,
            "bigbench_dyck_languages": 3.1182115898132325,
            "agi_eval_lsat_ar": 1.6639169957326807,
            "bigbench_cs_algorithms": 1.7640942419117147,
            "bigbench_logical_deduction": 0.950961989680926,
            "bigbench_operators": 4.043958842754364,
            "bigbench_repeat_copy_logic": 1.0463651772588491,
            "simple_arithmetic_nospaces": 5.961821620702744,
            "simple_arithmetic_withspaces": 5.146448135852814,
            "math_qa": 1.7956423450547339,
            "logi_qa": 1.7755692527041458,
            "pubmed_qa_labeled": 3.7729077608585357,
            "squad": 1.2778837994051182,
            "agi_eval_lsat_rc": 1.9041420550488715,
            "agi_eval_lsat_lr": 1.7363950519000784,
            "coqa": 1.0952097897158968,
            "bigbench_understanding_fables": 1.5346575565439053,
            "boolq": 2.9203801945444274,
            "agi_eval_sat_en": 1.971222964884008,
            "winogender_mc_female": 0.7855558956662814,
            "winogender_mc_male": 0.776621957619985,
            "enterprise_pii_classification": 4.382509054315985,
            "bbq": 0.23786689477817857,
            "human_eval_return_complex": 0.8766255172218863,
            "human_eval_return_simple": 2.096573811930579,
            "human_eval-0.5": 0.8114368848320914,
            "human_eval-0.25": 0.8396049279992174,
            "human_eval-0.75": 0.8380035075654344,
            "human_eval": 0.8925811379421048,
            "processed_human_eval_cpp": 1.0917750123124685,
            "processed_human_eval_js": 1.0467806938217907
        },
        "icl": {
            "mmlu_zeroshot": 0.30539784593540326,
            "hellaswag_zeroshot": 0.7403903603553772,
            "jeopardy": 0.46244717836380006,
            "triviaqa_sm_sub": 0.44966667890548706,
            "gsm8k": 0.02956785447895527,
            "agi_eval_sat_math": 0.0181818176060915,
            "aqua": 0.004081632476300001,
            "bigbench_qa_wikidata": 0.6939619183540344,
            "arc_easy": 0.7655723690986633,
            "arc_challenge": 0.4948805570602417,
            "mmlu_fewshot": 0.34186981173983794,
            "bigbench_misconceptions": 0.5844748616218567,
            "copa": 0.800000011920929,
            "siqa": 0.6325486302375793,
            "commonsense_qa": 0.4496314525604248,
            "piqa": 0.8057671189308167,
            "openbook_qa": 0.44600000977516174,
            "bigbench_novel_concepts": 0.59375,
            "bigbench_strange_stories": 0.7068965435028076,
            "bigbench_strategy_qa": 0.5945827960968018,
            "lambada_openai": 0.7234620451927185,
            "hellaswag": 0.7533360123634338,
            "winograd": 0.8681318759918213,
            "winogrande": 0.7079715728759766,
            "bigbench_conlang_translation": 0.03658536449074745,
            "bigbench_language_identification": 0.27309998869895935,
            "bigbench_conceptual_combinations": 0.41747573018074036,
            "bigbench_elementary_math_qa": 0.2589360475540161,
            "bigbench_dyck_languages": 0.1809999942779541,
            "agi_eval_lsat_ar": 0.28260868787765503,
            "bigbench_cs_algorithms": 0.43939393758773804,
            "bigbench_logical_deduction": 0.23666666448116302,
            "bigbench_operators": 0.24761904776096344,
            "bigbench_repeat_copy_logic": 0.09375,
            "simple_arithmetic_nospaces": 0.029999999329447746,
            "simple_arithmetic_withspaces": 0.03099999949336052,
            "math_qa": 0.26483404636383057,
            "logi_qa": 0.3102918565273285,
            "pubmed_qa_labeled": 0.49000000953674316,
            "squad": 0.5570482611656189,
            "agi_eval_lsat_rc": 0.25,
            "agi_eval_lsat_lr": 0.2705882489681244,
            "coqa": 0.4464487135410309,
            "bigbench_understanding_fables": 0.26455026865005493,
            "boolq": 0.7721712589263916,
            "agi_eval_sat_en": 0.3252427279949188,
            "winogender_mc_female": 0.5166666507720947,
            "winogender_mc_male": 0.5,
            "enterprise_pii_classification": 0.5101619958877563,
            "bbq": 0.480099927295338,
            "gpqa_main": 0.2410714328289032,
            "gpqa_diamond": 0.2222222238779068,
            "gsm8k_cot": 0.051554206758737564,
            "agi_eval_sat_math_cot": 0.027272727340459824,
            "aqua_cot": 0.008163264952600002,
            "svamp_cot": 0.18000000715255737
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.41614393739105165,
        "language understanding": 0.45760672481533865,
        "reading comprehension": 0.27821180624723957,
        "safety": 0.003464286977594544,
        "symbolic problem solving": 0.10991695443397924,
        "world knowledge": 0.293637781958831
    },
    "aggregated_centered_results": 0.2606545100308946,
    "aggregated_results": 0.41768420643691156,
    "rw_small": 0.7134976883729299,
    "rw_small_centered": 0.5140308166107935,
    "95%_CI_above": 0.5402840964109092,
    "95%_CI_above_centered": 0.3948298013035899,
    "99%_CI_above": 0.5535545386697934,
    "99%_CI_above_centered": 0.4406602692231252,
    "low_variance_datasets": 0.545667835121805,
    "low_variance_datasets_centered": 0.446599645972297,
    "model_uuid": "373ab764-1dbf-49e8-aad5-755ec40fd139",
    "_filename": "exp_data/evals/evaluation_rw_v2_cc_v3_f0.15_resiliparse_fasttext_openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train_0.1-open_lm_7b_swiglutorch-warm=5000-lr=0p003-wd=0p033-cd=3e-05-bs=2048-mult=1-seed=124-tokens=137788211200_heavy.json",
    "missing tasks": "[]",
    "Core": 0.446599645972297,
    "Extended": 0.2606545100308946
}