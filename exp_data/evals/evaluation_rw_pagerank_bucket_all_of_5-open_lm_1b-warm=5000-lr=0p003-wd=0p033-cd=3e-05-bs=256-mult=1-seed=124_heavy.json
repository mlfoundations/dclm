{
    "name": "/opt/ml/code/eval/heavy",
    "uuid": "f29c1a88-6caf-4380-9dd7-1630a7a5b9bc",
    "model": "open_lm_1b",
    "creation_date": "2024_02_06-01_07_09",
    "eval_metrics": {
        "perplexity": 2.6420242547988892,
        "downstream_perpexity": {
            "mmlu": 1.7897572626484848,
            "hellaswag": 2.341450040593515,
            "jeopardy_all": 2.0199858542596836,
            "triviaqa_sm_sub": 2.3885261352062224,
            "gsm8k": 2.0216997744370087,
            "agi_eval_sat_math": 1.8124472921544856,
            "aqua": 2.5060101246347233,
            "svamp": 2.725933005809784,
            "bigbench_qa_wikidata": 3.5916921924074643,
            "arc_easy": 2.622050412074484,
            "arc_challenge": 2.7086745678564794,
            "bigbench_misconceptions": 5.175191385016594,
            "copa": 2.554693319797516,
            "siqa": 1.2704305874532662,
            "commonsense_qa": 1.7111558903444994,
            "piqa": 2.670776886441892,
            "openbook_qa": 4.191150084495544,
            "bigbench_novel_concepts": 2.6987713873386383,
            "bigbench_strange_stories": 3.138928575762387,
            "bigbench_strategy_qa": 2.0113324390430543,
            "lambada_openai": 1.4934053491709212,
            "winograd_wsc": 2.5547429537161803,
            "winogrande": 3.1094216765996125,
            "bigbench_conlang_translation": 2.065177625272332,
            "bigbench_language_identification": 3.519694187732238,
            "bigbench_conceptual_combinations": 0.8860841758042863,
            "bigbench_elementary_math_qa": 3.179640552479641,
            "bigbench_dyck_languages": 4.2703945729732515,
            "agi_eval_lsat_ar": 1.7088709012321803,
            "bigbench_cs_algorithms": 4.445136531916532,
            "bigbench_logical_deduction": 0.7660514920552571,
            "bigbench_operators": 5.196958502133687,
            "bigbench_repeat_copy_logic": 1.4224199410527945,
            "simple_arithmetic_nospaces": 6.862207602977753,
            "simple_arithmetic_withspaces": 6.312757821559906,
            "math_qa": 3.478059644552033,
            "logi_qa": 1.9385865472611927,
            "pubmed_qa_labeled": 3.672251749753952,
            "squad": 1.9228934309904375,
            "agi_eval_lsat_rc": 1.7437325969560822,
            "agi_eval_lsat_lr": 1.762782463606666,
            "coqa": 2.1896686676703463,
            "bigbench_understanding_fables": 3.0232125186415577,
            "boolq": 2.8240598520372258,
            "agi_eval_sat_en": 1.787794999705935,
            "winogender_mc_female": 1.2162161548932393,
            "winogender_mc_male": 1.145573620001475,
            "enterprise_pii_classification": 4.685196711915231,
            "bbq": 0.3519534196355356,
            "human_eval_return_complex": 3.5568034855399544,
            "human_eval_return_simple": 6.865354963251062,
            "human_eval-0.5": 3.4506467857011933,
            "human_eval-0.25": 3.5903001558489915,
            "human_eval-0.75": 3.547299803757086,
            "human_eval": 3.842938309762536,
            "processed_human_eval_cpp": 3.396872837350976,
            "processed_human_eval_js": 2.8937313949189534
        },
        "icl": {
            "mmlu_zeroshot": 0.23111977467411443,
            "hellaswag_zeroshot": 0.5703046917915344,
            "jeopardy": 0.1629139557480812,
            "triviaqa_sm_sub": 0.00033333332976326346,
            "gsm8k": 0.0,
            "agi_eval_sat_math": 0.004545454401522875,
            "aqua": 0.004081632476300001,
            "svamp": 0.0,
            "bigbench_qa_wikidata": 0.6018404364585876,
            "arc_easy": 0.5904881954193115,
            "arc_challenge": 0.29522183537483215,
            "bigbench_misconceptions": 0.4794520437717438,
            "copa": 0.6700000166893005,
            "siqa": 0.5097236633300781,
            "commonsense_qa": 0.30384930968284607,
            "piqa": 0.7350381016731262,
            "openbook_qa": 0.3659999966621399,
            "bigbench_novel_concepts": 0.375,
            "bigbench_strange_stories": 0.517241358757019,
            "bigbench_strategy_qa": 0.506334662437439,
            "lambada_openai": 0.5361925363540649,
            "hellaswag": 0.5764787793159485,
            "winograd": 0.7435897588729858,
            "winogrande": 0.5706393122673035,
            "bigbench_conlang_translation": 0.018292682245373726,
            "bigbench_language_identification": 0.2578999996185303,
            "bigbench_conceptual_combinations": 0.2330097109079361,
            "bigbench_elementary_math_qa": 0.23309747874736786,
            "bigbench_dyck_languages": 0.1860000044107437,
            "agi_eval_lsat_ar": 0.2347826063632965,
            "bigbench_cs_algorithms": 0.459090918302536,
            "bigbench_logical_deduction": 0.24666666984558105,
            "bigbench_operators": 0.18095238506793976,
            "bigbench_repeat_copy_logic": 0.03125,
            "simple_arithmetic_nospaces": 0.0,
            "simple_arithmetic_withspaces": 0.0010000000474974513,
            "math_qa": 0.25812938809394836,
            "logi_qa": 0.2611367106437683,
            "pubmed_qa_labeled": 0.4869999885559082,
            "squad": 0.3441816568374634,
            "agi_eval_lsat_rc": 0.2238806039094925,
            "agi_eval_lsat_lr": 0.23725490272045135,
            "coqa": 0.2794688642024994,
            "bigbench_understanding_fables": 0.24338623881340027,
            "boolq": 0.6229357719421387,
            "agi_eval_sat_en": 0.24271844327449799,
            "winogender_mc_female": 0.4000000059604645,
            "winogender_mc_male": 0.5333333611488342,
            "enterprise_pii_classification": 0.4801178276538849,
            "bbq": 0.4518753235990351,
            "mmlu_fewshot": 0.24734348424693992
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.12899092204207352,
        "language understanding": 0.23888662870885313,
        "reading comprehension": 0.1310037262737751,
        "safety": -0.06733674081889066,
        "symbolic problem solving": 0.08168481791631854,
        "world knowledge": 0.1511945249208563
    },
    "aggregated_centered_results": 0.12403813722060437,
    "aggregated_results": 0.3560971657397819,
    "rw_small": 0.5532649705807368,
    "95%_CI_above": 0.42341761990123755,
    "99%_CI_above": 0.43601464998462924,
    "model_uuid": "08bf59b3-875c-4efc-ab76-00ab97976896",
    "low_variance_datasets": 0.4235963242297823,
    "_filename": "exp_data/evals/evaluation_rw_pagerank_bucket_all_of_5-open_lm_1b-warm=5000-lr=0p003-wd=0p033-cd=3e-05-bs=256-mult=1-seed=124_heavy.json",
    "missing tasks": "['gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'gpqa_main', 'gpqa_diamond']",
    "rw_small_centered": 0.2383928894996643,
    "95%_CI_above_centered": 0.2133397751746984,
    "99%_CI_above_centered": 0.26175390191902,
    "low_variance_datasets_centered": 0.2538017015044768,
    "Core": 0.2538017015044768,
    "Core_v1": 0.278016989647743,
    "Core_v2": 0.2538017015044768,
    "Extended_v2": "N/A due to missing tasks: ['gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'gpqa_main', 'gpqa_diamond']",
    "eval_version": "v2",
    "Extended_v1": "N/A due to missing tasks: ['gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'gpqa_main', 'gpqa_diamond']",
    "Extended": "N/A due to missing tasks: ['gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'gpqa_main', 'gpqa_diamond']"
}