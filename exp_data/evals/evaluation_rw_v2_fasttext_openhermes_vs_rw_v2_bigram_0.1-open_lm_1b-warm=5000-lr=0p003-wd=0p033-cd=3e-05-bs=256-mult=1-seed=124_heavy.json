{
    "name": "/opt/ml/code/eval/heavy",
    "uuid": "a60006f6-0cc5-402d-becf-d52e55b062b3",
    "model": "open_lm_1b",
    "creation_date": "2024_02_11-20_58_42",
    "eval_metrics": {
        "perplexity": 2.5730827927589415,
        "downstream_perpexity": {
            "mmlu": 1.5895821282775666,
            "hellaswag": 2.35325913366581,
            "jeopardy_all": 1.420046010048929,
            "triviaqa_sm_sub": 2.243148773640394,
            "gsm8k": 1.8056507317562551,
            "agi_eval_sat_math": 1.5228639494289051,
            "aqua": 2.231399539052224,
            "svamp": 2.618465136686961,
            "bigbench_qa_wikidata": 3.2124680507059002,
            "arc_easy": 2.295938421605211,
            "arc_challenge": 2.4303767235104132,
            "bigbench_misconceptions": 4.102473852296942,
            "copa": 2.4826745331287383,
            "siqa": 1.293795055901382,
            "commonsense_qa": 1.5959126263930112,
            "piqa": 2.6796835191361406,
            "openbook_qa": 4.100007447719574,
            "bigbench_novel_concepts": 2.4941460713744164,
            "bigbench_strange_stories": 3.621800744670561,
            "bigbench_strategy_qa": 1.8215871913925745,
            "lambada_openai": 1.3523367659813965,
            "winograd_wsc": 2.4693548334387194,
            "winogrande": 3.0864266436591143,
            "bigbench_conlang_translation": 1.910201938413992,
            "bigbench_language_identification": 4.031882071499825,
            "bigbench_conceptual_combinations": 1.1683355089530205,
            "bigbench_elementary_math_qa": 4.294774463310431,
            "bigbench_dyck_languages": 3.7308272935152056,
            "agi_eval_lsat_ar": 1.660441610087519,
            "bigbench_cs_algorithms": 5.480025147669243,
            "bigbench_logical_deduction": 1.239435686906179,
            "bigbench_operators": 4.799271933805375,
            "bigbench_repeat_copy_logic": 1.455428820103407,
            "simple_arithmetic_nospaces": 6.662741728782654,
            "simple_arithmetic_withspaces": 6.801813920021057,
            "math_qa": 4.38129760816998,
            "logi_qa": 1.6808476438903222,
            "pubmed_qa_labeled": 6.971448016643524,
            "squad": 2.2224324927481143,
            "agi_eval_lsat_rc": 1.6486788529958298,
            "agi_eval_lsat_lr": 1.6585677247421413,
            "coqa": 3.3253251172599376,
            "bigbench_understanding_fables": 2.8447867713908037,
            "boolq": 2.926038427236248,
            "agi_eval_sat_en": 1.655840051984324,
            "winogender_mc_female": 1.2094772468010584,
            "winogender_mc_male": 1.0808064917723337,
            "enterprise_pii_classification": 4.456114189052442,
            "bbq": 0.31642493178732584,
            "human_eval_return_complex": 2.3948834655791758,
            "human_eval_return_simple": 5.287158334577406,
            "human_eval-0.5": 2.289875502993421,
            "human_eval-0.25": 2.3840457644404434,
            "human_eval-0.75": 2.359670183280619,
            "human_eval": 2.516739877985745,
            "processed_human_eval_cpp": 2.5106277895269926,
            "processed_human_eval_js": 2.182922627140836
        },
        "icl": {
            "mmlu_zeroshot": 0.2538428693486933,
            "hellaswag_zeroshot": 0.5686118006706238,
            "jeopardy": 0.3036715656518936,
            "triviaqa_sm_sub": 0.00033333332976326346,
            "gsm8k": 0.0007581501267850399,
            "agi_eval_sat_math": 0.00909090880304575,
            "aqua": 0.0,
            "bigbench_qa_wikidata": 0.6184735298156738,
            "arc_easy": 0.6624578833580017,
            "arc_challenge": 0.3686006963253021,
            "bigbench_misconceptions": 0.47031962871551514,
            "copa": 0.7200000286102295,
            "siqa": 0.498976469039917,
            "commonsense_qa": 0.2702702581882477,
            "piqa": 0.747551679611206,
            "openbook_qa": 0.3700000047683716,
            "bigbench_novel_concepts": 0.4375,
            "bigbench_strange_stories": 0.5517241358757019,
            "bigbench_strategy_qa": 0.5124508738517761,
            "lambada_openai": 0.5940228700637817,
            "hellaswag": 0.5718980431556702,
            "winograd": 0.7655677795410156,
            "winogrande": 0.5698500275611877,
            "bigbench_conlang_translation": 0.012195121496915817,
            "bigbench_language_identification": 0.2549000084400177,
            "bigbench_conceptual_combinations": 0.3106796145439148,
            "bigbench_elementary_math_qa": 0.24166665971279144,
            "bigbench_dyck_languages": 0.2549999952316284,
            "agi_eval_lsat_ar": 0.28260868787765503,
            "bigbench_cs_algorithms": 0.4545454680919647,
            "bigbench_logical_deduction": 0.25,
            "bigbench_operators": 0.2142857164144516,
            "bigbench_repeat_copy_logic": 0.03125,
            "simple_arithmetic_nospaces": 0.004000000189989805,
            "simple_arithmetic_withspaces": 0.004999999888241291,
            "math_qa": 0.2534361481666565,
            "logi_qa": 0.25652840733528137,
            "pubmed_qa_labeled": 0.3569999933242798,
            "squad": 0.40085145831108093,
            "agi_eval_lsat_rc": 0.2723880708217621,
            "agi_eval_lsat_lr": 0.25294119119644165,
            "coqa": 0.30590003728866577,
            "bigbench_understanding_fables": 0.2698412835597992,
            "boolq": 0.608562707901001,
            "agi_eval_sat_en": 0.26213592290878296,
            "winogender_mc_female": 0.6333333253860474,
            "winogender_mc_male": 0.46666666865348816,
            "enterprise_pii_classification": 0.50044184923172,
            "bbq": 0.47357872941277246,
            "mmlu_fewshot": 0.26271479668324454,
            "gsm8k_cot": 0.01440485194325447,
            "agi_eval_sat_math_cot": 0.013636363670229912,
            "aqua_cot": 0.016326529905200005,
            "svamp_cot": 0.05999999865889549,
            "gpqa_main": 0.203125,
            "gpqa_diamond": 0.18686868250370026
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.15613481307548893,
        "language understanding": 0.2635954599063009,
        "reading comprehension": 0.13875774705880567,
        "safety": 0.037010286342013976,
        "symbolic problem solving": 0.078421052162432,
        "world knowledge": 0.14465975905202835
    },
    "aggregated_centered_results": 0.13658223921980348,
    "aggregated_results": 0.3442063540798575,
    "rw_small": 0.5830138574043909,
    "95%_CI_above": 0.4475771323620498,
    "99%_CI_above": 0.45832269917363705,
    "model_uuid": "d40a9cf2-f2e8-4439-9efd-6795c8a819ba",
    "low_variance_datasets": 0.45176728394898497,
    "_filename": "exp_data/evals/evaluation_rw_v2_fasttext_openhermes_vs_rw_v2_bigram_0.1-open_lm_1b-warm=5000-lr=0p003-wd=0p033-cd=3e-05-bs=256-mult=1-seed=124_heavy.json",
    "missing tasks": "[]",
    "rw_small_centered": 0.2804159692853515,
    "95%_CI_above_centered": 0.24488907828989723,
    "99%_CI_above_centered": 0.28934638429032306,
    "low_variance_datasets_centered": 0.2865280223398364,
    "Core": 0.2865280223398364,
    "Extended": 0.13658223921980348,
    "Core_v1": 0.3112427043085525,
    "Extended_v1": 0.1598558118769758,
    "Core_v2": 0.2865280223398364,
    "Extended_v2": 0.13658223921980348,
    "eval_version": "v2"
}