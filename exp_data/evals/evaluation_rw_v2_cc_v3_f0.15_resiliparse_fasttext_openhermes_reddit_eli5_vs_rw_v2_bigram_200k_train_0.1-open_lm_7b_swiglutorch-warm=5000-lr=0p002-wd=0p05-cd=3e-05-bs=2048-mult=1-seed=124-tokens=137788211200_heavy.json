{
    "name": "/opt/ml/code/eval/heavy",
    "uuid": "bb638925-5f9e-49a1-8718-10490f0cb717",
    "model": "open_lm_7b_swiglutorch",
    "creation_date": "2024_05_08-04_00_39",
    "eval_metrics": {
        "perplexity": 2.344316891829173,
        "downstream_perpexity": {
            "mmlu": 1.623260625703198,
            "hellaswag": 2.097427006345706,
            "jeopardy_all": 0.9102447975337168,
            "triviaqa_sm_sub": 1.481039291329682,
            "gsm8k": 1.3895739586268345,
            "agi_eval_sat_math": 1.2493143097920851,
            "aqua": 1.848631864664506,
            "svamp": 2.3800234166781107,
            "bigbench_qa_wikidata": 2.855383323138785,
            "arc_easy": 1.8463426304836867,
            "arc_challenge": 2.0276618479360087,
            "bigbench_misconceptions": 2.7387568036170853,
            "copa": 2.2284641456604004,
            "siqa": 1.2744986121461725,
            "commonsense_qa": 1.6467605511933248,
            "piqa": 2.277972833116633,
            "openbook_qa": 3.715712692260742,
            "bigbench_novel_concepts": 2.09610129147768,
            "bigbench_strange_stories": 2.621244036603248,
            "bigbench_strategy_qa": 1.661755812069171,
            "lambada_openai": 0.8885123217982642,
            "winograd_wsc": 2.2348085253229946,
            "winogrande": 2.864710305371243,
            "bigbench_conlang_translation": 1.5225152380582763,
            "bigbench_language_identification": 1.5922266321912912,
            "bigbench_conceptual_combinations": 0.8217306704197115,
            "bigbench_elementary_math_qa": 4.317474564332008,
            "bigbench_dyck_languages": 3.1475991432666777,
            "agi_eval_lsat_ar": 1.9174189173656961,
            "bigbench_cs_algorithms": 2.2722889203013796,
            "bigbench_logical_deduction": 0.9810214587450028,
            "bigbench_operators": 4.026078602245875,
            "bigbench_repeat_copy_logic": 0.9627361465245485,
            "simple_arithmetic_nospaces": 5.887479243040085,
            "simple_arithmetic_withspaces": 5.077255681991577,
            "math_qa": 1.7094938170378375,
            "logi_qa": 1.9338853791378976,
            "pubmed_qa_labeled": 5.292160656929016,
            "squad": 1.360170551094816,
            "agi_eval_lsat_rc": 1.9199643428645916,
            "agi_eval_lsat_lr": 1.8442085284812777,
            "coqa": 1.160704774537302,
            "bigbench_understanding_fables": 1.575096474122749,
            "boolq": 3.1576516618057857,
            "agi_eval_sat_en": 2.03663006338101,
            "winogender_mc_female": 0.7647614727417628,
            "winogender_mc_male": 0.6536652714014053,
            "enterprise_pii_classification": 4.289649103035456,
            "bbq": 0.27403503239562677,
            "human_eval_return_complex": 0.8872669400192621,
            "human_eval_return_simple": 2.0515662302842013,
            "human_eval-0.5": 0.8162417111055154,
            "human_eval-0.25": 0.8434039895854345,
            "human_eval-0.75": 0.8348230045379662,
            "human_eval": 0.9068654655683331,
            "processed_human_eval_cpp": 1.0766073376495646,
            "processed_human_eval_js": 1.0660766515789963
        },
        "icl": {
            "mmlu_zeroshot": 0.32816645869037564,
            "hellaswag_zeroshot": 0.7418841123580933,
            "jeopardy": 0.4688451290130615,
            "triviaqa_sm_sub": 0.46833333373069763,
            "gsm8k": 0.03790750727057457,
            "agi_eval_sat_math": 0.022727273404598236,
            "aqua": 0.004081632476300001,
            "svamp": 0.1599999964237213,
            "bigbench_qa_wikidata": 0.7108902335166931,
            "arc_easy": 0.7592592835426331,
            "arc_challenge": 0.49744027853012085,
            "mmlu_fewshot": 0.42228314855642485,
            "bigbench_misconceptions": 0.543379008769989,
            "copa": 0.800000011920929,
            "siqa": 0.690378725528717,
            "commonsense_qa": 0.6076986193656921,
            "piqa": 0.7954298257827759,
            "openbook_qa": 0.4560000002384186,
            "bigbench_novel_concepts": 0.5625,
            "bigbench_strange_stories": 0.7183908224105835,
            "bigbench_strategy_qa": 0.5954565405845642,
            "lambada_openai": 0.7195808291435242,
            "hellaswag": 0.7564229965209961,
            "winograd": 0.8498168587684631,
            "winogrande": 0.687450647354126,
            "bigbench_conlang_translation": 0.04268292710185051,
            "bigbench_language_identification": 0.301800012588501,
            "bigbench_conceptual_combinations": 0.3786407709121704,
            "bigbench_elementary_math_qa": 0.2601677179336548,
            "bigbench_dyck_languages": 0.19900000095367432,
            "agi_eval_lsat_ar": 0.24782608449459076,
            "bigbench_cs_algorithms": 0.4310606122016907,
            "bigbench_logical_deduction": 0.23866666853427887,
            "bigbench_operators": 0.2809523940086365,
            "bigbench_repeat_copy_logic": 0.125,
            "simple_arithmetic_nospaces": 0.029999999329447746,
            "simple_arithmetic_withspaces": 0.03200000151991844,
            "math_qa": 0.2604760229587555,
            "logi_qa": 0.3041474521160126,
            "pubmed_qa_labeled": 0.3959999978542328,
            "squad": 0.5430463552474976,
            "agi_eval_lsat_rc": 0.26865673065185547,
            "agi_eval_lsat_lr": 0.2921568751335144,
            "coqa": 0.4240260422229767,
            "bigbench_understanding_fables": 0.3968254029750824,
            "boolq": 0.7253822684288025,
            "agi_eval_sat_en": 0.3446601927280426,
            "winogender_mc_female": 0.5333333611488342,
            "winogender_mc_male": 0.550000011920929,
            "enterprise_pii_classification": 0.5410898327827454,
            "bbq": 0.4897720055146651,
            "gpqa_main": 0.2477678507566452,
            "gpqa_diamond": 0.2626262605190277
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.4489711913485443,
        "language understanding": 0.4464123932141667,
        "reading comprehension": 0.2767070632003117,
        "safety": 0.05709760568358682,
        "symbolic problem solving": 0.12761008700733983,
        "world knowledge": 0.30915510879274
    },
    "aggregated_centered_results": 0.2877628091805442,
    "aggregated_results": 0.45566062687479414,
    "rw_small": 0.7019027670224508,
    "rw_small_centered": 0.48616688502462285,
    "95%_CI_above": 0.5390935422634924,
    "95%_CI_above_centered": 0.3910449856986688,
    "99%_CI_above": 0.5549609745326249,
    "99%_CI_above_centered": 0.4394512672963305,
    "low_variance_datasets": 0.5513096634637226,
    "low_variance_datasets_centered": 0.4482319403892566,
    "model_uuid": "0d6f92aa-6cf7-4b64-8d4f-a55a55573e18",
    "_filename": "exp_data/evals/evaluation_rw_v2_cc_v3_f0.15_resiliparse_fasttext_openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train_0.1-open_lm_7b_swiglutorch-warm=5000-lr=0p002-wd=0p05-cd=3e-05-bs=2048-mult=1-seed=124-tokens=137788211200_heavy.json",
    "missing tasks": "['gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot']",
    "Core": 0.4482319403892566,
    "Extended": "N/A due to missing tasks: ['gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot']"
}