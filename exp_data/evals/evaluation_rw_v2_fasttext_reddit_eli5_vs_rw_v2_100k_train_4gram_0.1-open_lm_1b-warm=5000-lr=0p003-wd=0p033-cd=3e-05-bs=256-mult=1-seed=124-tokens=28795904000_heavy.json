{
    "name": "/mnt/task_runtime/dcnlp/eval/heavy",
    "uuid": "75924814-0a95-4438-87b0-dd7d6a0a37eb",
    "model": "open_lm_1b",
    "creation_date": "2024_02_20-11_50_09",
    "eval_metrics": {
        "perplexity": 2.6914789378643036,
        "downstream_perpexity": {
            "mmlu": 1.7301969289389147,
            "hellaswag": 2.3325336937457055,
            "jeopardy_all": 1.9287935936287657,
            "triviaqa_sm_sub": 2.558792944649855,
            "gsm8k": 1.8387967051657155,
            "agi_eval_sat_math": 1.5782160276716406,
            "aqua": 2.2711775332081077,
            "svamp": 2.552132511138916,
            "bigbench_qa_wikidata": 3.6970358857910943,
            "arc_easy": 2.241479837683716,
            "arc_challenge": 2.3990693228643503,
            "bigbench_misconceptions": 5.0313685952800595,
            "copa": 2.436283484697342,
            "siqa": 1.059621848432898,
            "commonsense_qa": 1.6486248360804903,
            "piqa": 2.6527773009550324,
            "openbook_qa": 3.992231017112732,
            "bigbench_novel_concepts": 2.537157282233238,
            "bigbench_strange_stories": 3.034146536355731,
            "bigbench_strategy_qa": 1.8122645338428442,
            "lambada_openai": 1.5085222934040714,
            "winograd_wsc": 2.5058367959745635,
            "winogrande": 3.0335033829981826,
            "bigbench_conlang_translation": 2.035841837888811,
            "bigbench_language_identification": 2.904757899030444,
            "bigbench_conceptual_combinations": 1.094451503267566,
            "bigbench_elementary_math_qa": 3.5362767901091954,
            "bigbench_dyck_languages": 4.571521600246429,
            "agi_eval_lsat_ar": 1.8043485387511875,
            "bigbench_cs_algorithms": 5.149964847347953,
            "bigbench_logical_deduction": 1.1045754472414653,
            "bigbench_operators": 4.931788528533209,
            "bigbench_repeat_copy_logic": 1.390595881268382,
            "simple_arithmetic_nospaces": 6.517485414981842,
            "simple_arithmetic_withspaces": 6.116787860393524,
            "math_qa": 3.3788254607894697,
            "logi_qa": 1.7033652438912341,
            "pubmed_qa_labeled": 4.2424581732749935,
            "squad": 2.000991710245102,
            "agi_eval_lsat_rc": 1.5957766473293304,
            "agi_eval_lsat_lr": 1.6643423896209866,
            "coqa": 2.7208888693051754,
            "bigbench_understanding_fables": 2.6866980620792935,
            "boolq": 3.188024265029744,
            "agi_eval_sat_en": 1.5632219395591218,
            "winogender_mc_female": 1.1161360681056975,
            "winogender_mc_male": 0.9892593562602997,
            "enterprise_pii_classification": 4.558759750849369,
            "bbq": 0.2956178086951649,
            "human_eval_return_complex": 2.456991398428369,
            "human_eval_return_simple": 5.690242367821771,
            "human_eval-0.5": 2.3962445891484982,
            "human_eval-0.25": 2.4908414395844063,
            "human_eval-0.75": 2.436207891964331,
            "human_eval": 2.605606346595578,
            "processed_human_eval_cpp": 2.665366982821352,
            "processed_human_eval_js": 2.3930300692232644
        },
        "icl": {
            "mmlu_zeroshot": 0.25847345956584866,
            "hellaswag_zeroshot": 0.5861382484436035,
            "jeopardy": 0.21548219621181489,
            "triviaqa_sm_sub": 0.00033333332976326346,
            "gsm8k": 0.0,
            "agi_eval_sat_math": 0.004545454401522875,
            "aqua": 0.0,
            "bigbench_qa_wikidata": 0.5542542338371277,
            "arc_easy": 0.6645622849464417,
            "arc_challenge": 0.3668941855430603,
            "bigbench_misconceptions": 0.4931506812572479,
            "copa": 0.7400000095367432,
            "siqa": 0.49283522367477417,
            "commonsense_qa": 0.20393119752407074,
            "piqa": 0.7431991100311279,
            "openbook_qa": 0.3959999978542328,
            "bigbench_novel_concepts": 0.46875,
            "bigbench_strange_stories": 0.5977011322975159,
            "bigbench_strategy_qa": 0.5513324737548828,
            "lambada_openai": 0.5330875515937805,
            "hellaswag": 0.5884286165237427,
            "winograd": 0.7435897588729858,
            "winogrande": 0.6061562895774841,
            "bigbench_conlang_translation": 0.024390242993831635,
            "bigbench_language_identification": 0.2565999925136566,
            "bigbench_conceptual_combinations": 0.24271844327449799,
            "bigbench_elementary_math_qa": 0.23417191207408905,
            "bigbench_dyck_languages": 0.18799999356269836,
            "agi_eval_lsat_ar": 0.22173912823200226,
            "bigbench_cs_algorithms": 0.45681819319725037,
            "bigbench_logical_deduction": 0.25999999046325684,
            "bigbench_operators": 0.19523809850215912,
            "bigbench_repeat_copy_logic": 0.0,
            "simple_arithmetic_nospaces": 0.0010000000474974513,
            "simple_arithmetic_withspaces": 0.003000000026077032,
            "math_qa": 0.2601408064365387,
            "logi_qa": 0.24731183052062988,
            "pubmed_qa_labeled": 0.5540000200271606,
            "squad": 0.369536429643631,
            "agi_eval_lsat_rc": 0.25746268033981323,
            "agi_eval_lsat_lr": 0.2666666805744171,
            "coqa": 0.2749592959880829,
            "bigbench_understanding_fables": 0.190476194024086,
            "boolq": 0.6079510450363159,
            "agi_eval_sat_en": 0.23786407709121704,
            "winogender_mc_female": 0.5666666626930237,
            "winogender_mc_male": 0.46666666865348816,
            "enterprise_pii_classification": 0.45243003964424133,
            "bbq": 0.5141795873641968,
            "mmlu_fewshot": 0.24855050784454014,
            "gsm8k_cot": 0.011372251436114311,
            "agi_eval_sat_math_cot": 0.00909090880304575,
            "aqua_cot": 0.004081632476300001,
            "svamp_cot": 0.06666667014360428,
            "gpqa_main": 0.21875,
            "gpqa_diamond": 0.2070707082748413
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.2549520788692414,
        "language understanding": 0.291107730674624,
        "reading comprehension": 0.16173296032081308,
        "safety": -2.8520822525024414e-05,
        "symbolic problem solving": 0.07495019069891934,
        "world knowledge": 0.13754393207928445
    },
    "aggregated_centered_results": 0.15399849541328234,
    "aggregated_results": 0.33811076747695384,
    "rw_small": 0.5952836771806082,
    "95%_CI_above": 0.4388102949729987,
    "99%_CI_above": 0.44619112714477205,
    "low_variance_datasets": 0.43238935714418236,
    "model_uuid": "7a5cb0f0-659f-45b0-8b32-1abb2a097694",
    "_filename": "exp_data/evals/evaluation_rw_v2_fasttext_reddit_eli5_vs_rw_v2_100k_train_4gram_0.1-open_lm_1b-warm=5000-lr=0p003-wd=0p033-cd=3e-05-bs=256-mult=1-seed=124-tokens=28795904000_heavy.json",
    "missing tasks": "[]",
    "rw_small_centered": 0.3028996300976179,
    "95%_CI_above_centered": 0.2550491975832508,
    "99%_CI_above_centered": 0.29196540593964554,
    "low_variance_datasets_centered": 0.29266011122191754,
    "Core": 0.29266011122191754,
    "Extended": 0.15399849541328234
}