{
    "name": "/opt/ml/code/eval/heavy",
    "uuid": "b6b79c3b-a095-45d2-aa16-26477f4d95ad",
    "model": "open_lm_7b_swiglutorch",
    "creation_date": "2024_05_09-16_41_13",
    "eval_metrics": {
        "perplexity": 2.143852546811104,
        "downstream_perpexity": {
            "mmlu": 1.707978145703684,
            "hellaswag": 2.114021742813997,
            "jeopardy_all": 0.8817030206431772,
            "triviaqa_sm_sub": 1.5060037560587127,
            "gsm8k": 1.3584214363973008,
            "agi_eval_sat_math": 1.2086226593364369,
            "aqua": 1.783051743312758,
            "svamp": 2.3106255380312604,
            "bigbench_qa_wikidata": 3.109199840809559,
            "arc_easy": 1.8618125333278268,
            "arc_challenge": 2.0439439243525777,
            "bigbench_misconceptions": 2.3397451089397414,
            "copa": 2.227101719379425,
            "siqa": 1.1939016029986047,
            "commonsense_qa": 1.743036226792769,
            "piqa": 2.285669308946234,
            "openbook_qa": 3.768025415897369,
            "bigbench_novel_concepts": 2.136091336607933,
            "bigbench_strange_stories": 2.546933339245018,
            "bigbench_strategy_qa": 1.706926359261629,
            "lambada_openai": 0.9245469139069434,
            "winograd_wsc": 2.2681476410492,
            "winogrande": 2.900854752961349,
            "bigbench_conlang_translation": 1.409363089174759,
            "bigbench_language_identification": 1.5594307547165027,
            "bigbench_conceptual_combinations": 0.9605706484572402,
            "bigbench_elementary_math_qa": 3.344220513071029,
            "bigbench_dyck_languages": 3.3909854763150213,
            "agi_eval_lsat_ar": 1.6987870962723441,
            "bigbench_cs_algorithms": 1.8126355423168703,
            "bigbench_logical_deduction": 0.9988435386816661,
            "bigbench_operators": 3.7302705032484873,
            "bigbench_repeat_copy_logic": 0.9988699033856392,
            "simple_arithmetic_nospaces": 5.914909636020661,
            "simple_arithmetic_withspaces": 4.905419379234314,
            "math_qa": 1.6852367434212316,
            "logi_qa": 1.8017568441763085,
            "pubmed_qa_labeled": 3.928814961194992,
            "squad": 1.3045624985113782,
            "agi_eval_lsat_rc": 2.095418491025469,
            "agi_eval_lsat_lr": 1.738566178667779,
            "coqa": 1.1073370815204466,
            "bigbench_understanding_fables": 1.4628618299645721,
            "boolq": 3.166416116300344,
            "agi_eval_sat_en": 2.088604454855317,
            "winogender_mc_female": 0.7997823084394137,
            "winogender_mc_male": 0.6727296322584152,
            "enterprise_pii_classification": 3.287009266655294,
            "bbq": 0.265511668202798,
            "human_eval_return_complex": 0.579150724129414,
            "human_eval_return_simple": 3.4584188396866256,
            "human_eval-0.5": 0.4926885824377944,
            "human_eval-0.25": 0.5342137370167709,
            "human_eval-0.75": 0.5056772967119042,
            "human_eval": 0.5942531157194114,
            "processed_human_eval_cpp": 0.5837902367485236,
            "processed_human_eval_js": 0.5997500635865258
        },
        "icl": {
            "mmlu_zeroshot": 0.24391380788987144,
            "hellaswag_zeroshot": 0.7259510159492493,
            "jeopardy": 0.45426705479621887,
            "triviaqa_sm_sub": 0.4423333406448364,
            "gsm8k": 0.04397270828485489,
            "agi_eval_sat_math": 0.022727273404598236,
            "aqua": 0.020408162847161293,
            "bigbench_qa_wikidata": 0.6945524215698242,
            "arc_easy": 0.7575757503509521,
            "arc_challenge": 0.49317407608032227,
            "mmlu_fewshot": 0.30123487585469294,
            "bigbench_misconceptions": 0.4931506812572479,
            "copa": 0.8299999833106995,
            "siqa": 0.6079836487770081,
            "commonsense_qa": 0.2891072928905487,
            "piqa": 0.7959738969802856,
            "openbook_qa": 0.4440000057220459,
            "bigbench_novel_concepts": 0.5625,
            "bigbench_strange_stories": 0.6609195470809937,
            "bigbench_strategy_qa": 0.5596330165863037,
            "lambada_openai": 0.7114302515983582,
            "hellaswag": 0.7405894994735718,
            "winograd": 0.8498168587684631,
            "winogrande": 0.675611674785614,
            "bigbench_conlang_translation": 0.04268292710185051,
            "bigbench_language_identification": 0.26350000500679016,
            "bigbench_conceptual_combinations": 0.3300970792770386,
            "bigbench_elementary_math_qa": 0.2631027400493622,
            "bigbench_dyck_languages": 0.18400000035762787,
            "agi_eval_lsat_ar": 0.2956521809101105,
            "bigbench_cs_algorithms": 0.4598484933376312,
            "bigbench_logical_deduction": 0.2566666603088379,
            "bigbench_operators": 0.27142858505249023,
            "bigbench_repeat_copy_logic": 0.09375,
            "simple_arithmetic_nospaces": 0.02800000086426735,
            "simple_arithmetic_withspaces": 0.027000000700354576,
            "math_qa": 0.2567884624004364,
            "logi_qa": 0.2611367106437683,
            "pubmed_qa_labeled": 0.5289999842643738,
            "squad": 0.5286660194396973,
            "agi_eval_lsat_rc": 0.24253731966018677,
            "agi_eval_lsat_lr": 0.24901960790157318,
            "coqa": 0.4276587665081024,
            "bigbench_understanding_fables": 0.25925925374031067,
            "boolq": 0.6920489072799683,
            "agi_eval_sat_en": 0.29611650109291077,
            "winogender_mc_female": 0.46666666865348816,
            "winogender_mc_male": 0.5,
            "enterprise_pii_classification": 0.5522827506065369,
            "bbq": 0.496695412830873,
            "gpqa_main": 0.2410714328289032,
            "gpqa_diamond": 0.23232322931289673,
            "gsm8k_cot": 0.04473085701465607,
            "agi_eval_sat_math_cot": 0.022727273404598236,
            "aqua_cot": 0.020408162847161293,
            "svamp_cot": 0.1133333370089531
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.3660622521027595,
        "language understanding": 0.4237820026434024,
        "reading comprehension": 0.24061700656781332,
        "safety": 0.007822416045449004,
        "symbolic problem solving": 0.1062231848603242,
        "world knowledge": 0.2603178409282227
    },
    "aggregated_centered_results": 0.2353114327713338,
    "aggregated_results": 0.4015456232221295,
    "rw_small": 0.6957269012928009,
    "rw_small_centered": 0.47273780449092045,
    "95%_CI_above": 0.5252302746203813,
    "95%_CI_above_centered": 0.3711195442152498,
    "99%_CI_above": 0.5433211073927258,
    "99%_CI_above_centered": 0.4230830767721188,
    "low_variance_datasets": 0.5308455790985714,
    "low_variance_datasets_centered": 0.4221415573948634,
    "model_uuid": "346c0af9-cf81-4bd1-8af2-645a15ef135d",
    "_filename": "exp_data/evals/evaluation_hero-run1-2x-starcoder-math_datasets-open_lm_7b_swiglutorch-warm=5000-lr=0p001-wd=0p1-cd=3e-05-bs=2048-mult=1-seed=124-tokens=137788211200_heavy.json",
    "missing tasks": "[]",
    "Core": 0.4221415573948634,
    "Extended": 0.2353114327713338
}