{
    "name": "/opt/ml/code/eval/heavy",
    "uuid": "2c276601-af7a-412d-b364-2370cdb392ba",
    "model": "open_lm_1b_swiglutorch",
    "creation_date": "2024_04_29-11_34_24",
    "eval_metrics": {
        "perplexity": 2.743312998612722,
        "downstream_perpexity": {
            "mmlu": 1.6697792547531165,
            "hellaswag": 2.326039821330923,
            "jeopardy_all": 1.6944236326918718,
            "triviaqa_sm_sub": 2.320582520698508,
            "gsm8k": 1.852490151262898,
            "agi_eval_sat_math": 1.5440559024160558,
            "aqua": 2.28650708977057,
            "svamp": 2.5356833720207215,
            "bigbench_qa_wikidata": 3.5283089937778516,
            "arc_easy": 2.2416617328470405,
            "arc_challenge": 2.4215385056396395,
            "bigbench_misconceptions": 5.221435098343244,
            "copa": 2.4933200764656065,
            "siqa": 1.4880189217222872,
            "commonsense_qa": 1.698999007152398,
            "piqa": 2.539523593359855,
            "openbook_qa": 4.102118850708008,
            "bigbench_novel_concepts": 2.414500042796135,
            "bigbench_strange_stories": 2.7429701596841043,
            "bigbench_strategy_qa": 1.737306019060994,
            "lambada_openai": 1.2708960449568496,
            "winograd_wsc": 2.4927479343099908,
            "winogrande": 3.0554597535844668,
            "bigbench_conlang_translation": 2.076583420358053,
            "bigbench_language_identification": 1.5091271524931054,
            "bigbench_conceptual_combinations": 1.2041346164583002,
            "bigbench_elementary_math_qa": 3.9035025065112663,
            "bigbench_dyck_languages": 5.0464799304008485,
            "agi_eval_lsat_ar": 1.648689169469087,
            "bigbench_cs_algorithms": 4.477453747662631,
            "bigbench_logical_deduction": 1.0545630631049474,
            "bigbench_operators": 5.097445934159415,
            "bigbench_repeat_copy_logic": 1.3828977402299643,
            "simple_arithmetic_nospaces": 6.686697690725326,
            "simple_arithmetic_withspaces": 5.580480806350708,
            "math_qa": 1.6304089235771912,
            "logi_qa": 1.7485562304747269,
            "pubmed_qa_labeled": 2.557167063474655,
            "squad": 1.7756164821680696,
            "agi_eval_lsat_rc": 1.9463830794861068,
            "agi_eval_lsat_lr": 1.6480272141157413,
            "coqa": 1.7170959201639164,
            "bigbench_understanding_fables": 1.4628769922508764,
            "boolq": 2.4861721132873393,
            "agi_eval_sat_en": 1.9661842411004224,
            "winogender_mc_female": 1.0798673570156097,
            "winogender_mc_male": 0.9597360759973526,
            "enterprise_pii_classification": 5.017342044116822,
            "bbq": 0.3035698967539286,
            "human_eval_return_complex": 1.362536110746579,
            "human_eval_return_simple": 2.8875612439336003,
            "human_eval-0.5": 1.2913691190684713,
            "human_eval-0.25": 1.2954347889597824,
            "human_eval-0.75": 1.354840867403077,
            "human_eval": 1.3598351318661759,
            "processed_human_eval_cpp": 1.5743305883052185,
            "processed_human_eval_js": 1.3645076838935293
        },
        "icl": {
            "mmlu_zeroshot": 0.24326525550139577,
            "hellaswag_zeroshot": 0.5786696076393127,
            "jeopardy": 0.24783725291490555,
            "triviaqa_sm_sub": 0.19433332979679108,
            "gsm8k": 0.009097801521420479,
            "agi_eval_sat_math": 0.013636363670229912,
            "aqua": 0.004081632476300001,
            "bigbench_qa_wikidata": 0.5966241955757141,
            "arc_easy": 0.6574074029922485,
            "arc_challenge": 0.361774742603302,
            "mmlu_fewshot": 0.2591523264060941,
            "bigbench_misconceptions": 0.4885844886302948,
            "copa": 0.699999988079071,
            "siqa": 0.4790174067020416,
            "commonsense_qa": 0.3398853540420532,
            "piqa": 0.7431991100311279,
            "openbook_qa": 0.4099999964237213,
            "bigbench_novel_concepts": 0.4375,
            "bigbench_strange_stories": 0.5804597735404968,
            "bigbench_strategy_qa": 0.5286151170730591,
            "lambada_openai": 0.6035318970680237,
            "hellaswag": 0.5889264941215515,
            "winograd": 0.7728937864303589,
            "winogrande": 0.5935280323028564,
            "bigbench_conlang_translation": 0.024390242993831635,
            "bigbench_language_identification": 0.24549999833106995,
            "bigbench_conceptual_combinations": 0.223300963640213,
            "bigbench_elementary_math_qa": 0.25005242228507996,
            "bigbench_dyck_languages": 0.15299999713897705,
            "agi_eval_lsat_ar": 0.208695650100708,
            "bigbench_cs_algorithms": 0.3916666805744171,
            "bigbench_logical_deduction": 0.23133333027362823,
            "bigbench_operators": 0.18571428954601288,
            "bigbench_repeat_copy_logic": 0.0625,
            "simple_arithmetic_nospaces": 0.0010000000474974513,
            "simple_arithmetic_withspaces": 0.004000000189989805,
            "math_qa": 0.2544418275356293,
            "logi_qa": 0.2626728117465973,
            "pubmed_qa_labeled": 0.5019999742507935,
            "squad": 0.369820237159729,
            "agi_eval_lsat_rc": 0.21641790866851807,
            "agi_eval_lsat_lr": 0.2568627595901489,
            "coqa": 0.29976198077201843,
            "bigbench_understanding_fables": 0.2063492089509964,
            "boolq": 0.601529061794281,
            "agi_eval_sat_en": 0.25242719054222107,
            "winogender_mc_female": 0.4833333194255829,
            "winogender_mc_male": 0.5166666507720947,
            "enterprise_pii_classification": 0.496023565530777,
            "bbq": 0.46025284041057934,
            "gpqa_main": 0.2232142835855484,
            "gpqa_diamond": 0.1818181872367859,
            "gsm8k_cot": 0.005307050887495279,
            "agi_eval_sat_math_cot": 0.027272727340459824,
            "aqua_cot": 0.004081632476300001,
            "svamp_cot": 0.0533333346247673
        }
    },
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.2510662062865296,
        "language understanding": 0.2981577741158399,
        "reading comprehension": 0.1532185075788252,
        "safety": -0.021861811930483033,
        "symbolic problem solving": 0.07056616709246534,
        "world knowledge": 0.15848066866485
    },
    "aggregated_centered_results": 0.15425267639930385,
    "aggregated_results": 0.34075369219428614,
    "rw_small": 0.582151472568512,
    "rw_small_centered": 0.27815288549278216,
    "95%_CI_above": 0.44204753678437175,
    "95%_CI_above_centered": 0.256626545481623,
    "99%_CI_above": 0.4549995346561722,
    "99%_CI_above_centered": 0.30245949208031714,
    "low_variance_datasets": 0.44147571616552095,
    "low_variance_datasets_centered": 0.3014767366999368,
    "model_uuid": "12ba5f94-bdef-473f-b081-1bd8420f1590",
    "_filename": "exp_data/evals/evaluation_cc_v4_resiliparse_rw_v2_bff_minngram20_10shards_shard3_OH_eli5_vs_rw_v2_bigram_200k_train_0.1-open_lm_1b_swiglutorch-warm=5000-lr=0p003-wd=0p033-cd=3e-05-bs=256-mult=1-seed=124-tokens=28795904000_heavy.json",
    "missing tasks": "[]",
    "Core": 0.3014767366999368,
    "Extended": 0.15425267639930385
}